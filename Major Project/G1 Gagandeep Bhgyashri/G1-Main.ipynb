{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "802a0441",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "# Import Torch libraries to facilitate model creation and training process\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "# Import torchvission to facilitate image dataset reading\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# \n",
    "import os\n",
    "import argparse\n",
    "\n",
    "#from utils import progress_bar\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from torch.nn.modules.module import _addindent\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "import scipy.cluster.hierarchy as hcluster\n",
    "import scipy.cluster.hierarchy as hac\n",
    "import scipy.cluster.hierarchy as fclusterdata\n",
    "import time\n",
    "from sklearn.preprocessing import normalize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a597bc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "??torch.nn.modules.module._addindent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfa128a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing the model summary alsong with number of parameter in the model in specefic format in the tempstr\n",
    "def torch_summarize(model, show_weights=True, show_parameters=True):\n",
    "    \"\"\"Summarizes torch model by showing trainable parameters and weights.\"\"\"\n",
    "    tmpstr = model.__class__.__name__ + ' (\\n'\n",
    "    total =0\n",
    "    for key, module in model._modules.items():\n",
    "        # if it contains layers let call it recursively to get params and weights\n",
    "        if type(module) in [torch.nn.modules.container.Container, torch.nn.modules.container.Sequential]:\n",
    "            modstr = torch_summarize(module)\n",
    "        else:\n",
    "            modstr = module.__repr__()\n",
    "        modstr = _addindent(modstr, 2)\n",
    "\n",
    "        params = sum([np.prod(p.size()) for p in module.parameters()])\n",
    "        weights = tuple([tuple(p.size()) for p in module.parameters()])\n",
    "\n",
    "\n",
    "        tmpstr += '  (' + key + '): ' + modstr\n",
    "        if show_weights:\n",
    "            tmpstr += ', weights={}'.format(weights)\n",
    "        if show_parameters:\n",
    "            tmpstr +=  ', parameters={}'.format(params)\n",
    "            total+=params\n",
    "            print(params)\n",
    "            print('total is ',total)\n",
    "        tmpstr += '\\n'\n",
    "\n",
    "    tmpstr = tmpstr + ')'\n",
    "    return tmpstr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2134b45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        inputs, targets = Variable(inputs), Variable(targets)\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.data.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets.data).cpu().sum()\n",
    "\n",
    "        progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "            % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbbed9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch, num_total, sum_total):\n",
    "    global best_acc\n",
    "    # best_acc = 0.0\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = Variable(inputs, volatile=True), Variable(targets)\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        test_loss += loss.data.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets.data).cpu().sum()\n",
    "\n",
    "        progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "            % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "        num_total+=1\n",
    "        sum_total+=100.*correct/total\n",
    "    \n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        print('Saving weight')\n",
    "        state = {\n",
    "            'net': net.module if use_cuda else net,\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir('checkpoint'):\n",
    "            os.mkdir('checkpoint')\n",
    "        torch.save(state, './checkpoint/ckpt_pruned_retrain.t7')\n",
    "        best_acc = acc\n",
    "\n",
    "    return best_acc, num_total, sum_total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1651ff82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_weights_agglo(weight, threshold, average=True,cosine=True,euclidean=False,chebyshev=False,manhattan=False):\n",
    "    t0 = time.time()\n",
    "    weight = weight.T\n",
    "    weight = normalize(weight, norm='l2', axis=1)\n",
    "    threshold =  1.0-threshold   # Conversion to distance measure\n",
    "    if cosine==True:\n",
    "        clusters = hcluster.fclusterdata(weight, threshold, criterion=\"distance\", metric='cosine', depth=1, method='centroid')\n",
    "        z = hac.linkage(weight, metric='cosine', method='complete')\n",
    "    elif euclidean==True:\n",
    "        clusters = hcluster.fclusterdata(weight, threshold, criterion=\"distance\", metric='euclidean', depth=1, method='centroid')\n",
    "        z = hac.linkage(weight, metric='euclidean', method='complete')\n",
    "    elif chebyshev==True:\n",
    "        clusters = hcluster.fclusterdata(weight, threshold, criterion=\"distance\", metric='chebyshev', depth=1, method='centroid')\n",
    "        z = hac.linkage(weight, metric='chebyshev', method='complete')\n",
    "    elif manhattan==True:\n",
    "        clusters = hcluster.fclusterdata(weight, threshold, criterion=\"distance\", metric='cityblock', depth=1, method='centroid')\n",
    "        z = hac.linkage(weight, metric='cityblock', method='complete')\n",
    "    \n",
    "    labels = hac.fcluster(z, threshold, criterion=\"distance\")\n",
    "\n",
    "    labels_unique = np.unique(labels)\n",
    "    n_clusters_ = len(labels_unique)\n",
    "\n",
    "    #print(n_clusters_)\n",
    "    elapsed_time = time.time() - t0\n",
    "    # print(elapsed_time)\n",
    "\n",
    "    a=np.array(labels)\n",
    "    sort_idx = np.argsort(a)\n",
    "    a_sorted = a[sort_idx]\n",
    "    unq_first = np.concatenate(([True], a_sorted[1:] != a_sorted[:-1]))\n",
    "    unq_items = a_sorted[unq_first]\n",
    "    unq_count = np.diff(np.nonzero(unq_first)[0])\n",
    "    unq_idx= np.split(sort_idx, np.cumsum(unq_count))\n",
    "    first_ele = [unq_idx[idx][-1] for idx in range(len(unq_idx))]\n",
    "    return n_clusters_, first_ele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551e9ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg =  [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M',512]\n",
    "\n",
    "class VGG19X(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG19X, self).__init__()\n",
    "        self.features = self._make_layers(cfg)\n",
    "        self.classifier = nn.Linear(cfg[-1], 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "    def _make_layers(self, cfg):\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for x in cfg[:-1]:\n",
    "            if x == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
    "                           nn.BatchNorm2d(x),\n",
    "                           nn.ReLU(inplace=True)]\n",
    "                in_channels = x\n",
    "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
    "        return nn.Sequential(*layers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5e33be",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch CIFAR10 Training')\n",
    "parser.add_argument('--lr', default=0.001, type=float, help='learning rate')\n",
    "parser.add_argument('--nbepochs', default=100, type=int, help='number of epochs')\n",
    "args = parser.parse_args()\n",
    "\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d50390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "print('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "nb_remanining_filters_all = []\n",
    "test_acc_c1= []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019201a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine=False\n",
    "euclidean=False\n",
    "chebyshev=False\n",
    "manhattan=False\n",
    "n=int(input(\"Entre 1 for cosine \\nEntre 2 for euclidean \\nEntre 3 for chebyshev \\nEntre 4 for manhattan \\nEnter the Choice: \"))\n",
    "if n==1:\n",
    "    cosine=True\n",
    "elif n==2:\n",
    "    euclidean=True \n",
    "elif n==3:\n",
    "    chebyshev=True\n",
    "elif n==4:\n",
    "    manhattan=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b6041a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#threshold=float(input(\"Entre threshold\"))  \n",
    "tau_values= [0.50, 0.55, 0.60, 0.65, 0.70, 0.75, 0.80 0.85, 0.90] #[threshold]\n",
    "    \n",
    "num_total, sum_total = 0, 0\n",
    "\n",
    "for threshold in tau_values:\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n",
    "    checkpoint = torch.load('./checkpoint/ckpt.t7')\n",
    "    net = checkpoint['net']\n",
    "    best_acc = checkpoint['acc']\n",
    "    print('best_acc is ', best_acc)\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    best_acc = 0.\n",
    "\n",
    "    print('==> Constructing pruned network..')\n",
    "    # print(torch_summarize(net))\n",
    "    for i, (name, module) in enumerate(net._modules.items()):\n",
    "        module = recursion_change_bn(module)\n",
    "    ii = 0\n",
    "    first_ele = None\n",
    "    nb_remanining_filters = []\n",
    "    total_flop_after_pruning = 0\n",
    "    rr = 1\n",
    "    for layer in net.modules():\n",
    "        #print(layer)\n",
    "        if isinstance(layer, nn.ReLU):\n",
    "            rr+=1\n",
    "        if isinstance(layer, nn.MaxPool2d):\n",
    "            rr+=1\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            weight = layer.weight.data.cpu().numpy()\n",
    "            bias = layer.bias.data.cpu().numpy()\n",
    "            if first_ele is not None:\n",
    "                weight_layers_rearranged = np.transpose(weight, (1, 0, 2, 3))\n",
    "                weight_layers_rearranged_pruned = weight_layers_rearranged[first_ele]\n",
    "                weight_layers_rearranged_pruned = np.transpose(weight_layers_rearranged_pruned, (1, 0, 2, 3))\n",
    "            else:\n",
    "                weight_layers_rearranged_pruned = weight\n",
    "\n",
    "            weight_layers_rearranged = np.reshape(weight_layers_rearranged_pruned, [weight_layers_rearranged_pruned.shape[0], -1])\n",
    "            n_clusters_,first_ele = cluster_weights_agglo(weight_layers_rearranged.T, threshold,cosine=cosine,euclidean=euclidean,chebyshev=chebyshev,manhattan=manhattan)\n",
    "            first_ele = sorted(first_ele)\n",
    "            nb_remanining_filters.append(n_clusters_)\n",
    "\n",
    "            weight_pruned = weight_layers_rearranged[first_ele]\n",
    "            bias_pruned = bias[first_ele]\n",
    "            weight_pruned = np.reshape(weight_pruned, [n_clusters_, weight_layers_rearranged_pruned.shape[1],weight_layers_rearranged_pruned.shape[2],weight_layers_rearranged_pruned.shape[3]])\n",
    "\n",
    "            params_1 = np.shape(weight_pruned)\n",
    "            layer.out_channels = params_1[0]\n",
    "            layer.in_channels = params_1[1]\n",
    "\n",
    "            weight_tensor = torch.from_numpy(weight_pruned)\n",
    "            bias_tensor = torch.from_numpy(bias_pruned)\n",
    "            layer.weight = torch.nn.Parameter(weight_tensor)\n",
    "            layer.bias = torch.nn.Parameter(bias_tensor)\n",
    "\n",
    "            params_1 = np.shape(weight_pruned)\n",
    "            C1_1 = int(params_1[0])\n",
    "            C2_1 = int(params_1[1])\n",
    "            K1_1 = int(params_1[2])\n",
    "            K2_1 = int(params_1[3])\n",
    "            x = Variable(torch.randn(1,3, 32, 32))\n",
    "            nett_1 = nn.Sequential(*list(net.features.children())[:rr])\n",
    "            out_1 = nett_1(x)\n",
    "            img_size_1 = out_1.size()\n",
    "            # print('feature map size is:', img_size_1)\n",
    "            # print('weight size is:', params_1)\n",
    "\n",
    "            H_1 = img_size_1[2]\n",
    "            W_1 = img_size_1[3]\n",
    "            if ii==0:\n",
    "                H_1 = 32\n",
    "                W_1 = 32\n",
    "\n",
    "            flops_1 = C1_1*C2_1*K1_1*K2_1*H_1*W_1\n",
    "            print('flop is ',flops_1, '\\n')\n",
    "            total_flop_after_pruning +=flops_1\n",
    "            # print(ii)\n",
    "            ii+=1\n",
    "            rr+=1\n",
    "\n",
    "        if isinstance(layer, nn.BatchNorm2d) and first_ele is not None:\n",
    "            bnorm_weight = layer.weight.data.cpu().numpy()\n",
    "            bnorm_weight = bnorm_weight[first_ele]\n",
    "            bnorm_bias = layer.bias.data.cpu().numpy()\n",
    "            bnorm_bias = bnorm_bias[first_ele]\n",
    "\n",
    "            bnorm_tensor = torch.from_numpy(bnorm_weight)\n",
    "            bias_tensor = torch.from_numpy(bnorm_bias)\n",
    "            layer.weight = torch.nn.Parameter(bnorm_tensor)\n",
    "            layer.bias = torch.nn.Parameter(bias_tensor)\n",
    "\n",
    "            layer.num_features = int(np.shape(bnorm_weight)[0])\n",
    "            bnorm_rm = layer.running_mean.cpu().numpy()\n",
    "            bnorm_rm = bnorm_rm[first_ele]\n",
    "            bnorm_rv = layer.running_var.cpu().numpy()\n",
    "            bnorm_rv = bnorm_rv[first_ele]\n",
    "            running_mean = torch.from_numpy(bnorm_rm)\n",
    "            layer.running_mean = running_mean\n",
    "            running_var = torch.from_numpy(bnorm_rv)\n",
    "            layer.running_var = running_var\n",
    "            rr+=1\n",
    "\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            weight_linear = layer.weight.data.cpu().numpy()\n",
    "            weight_linear_rearranged = np.transpose(weight_linear, (1, 0))\n",
    "            weight_linear_rearranged_pruned = weight_linear_rearranged[first_ele]\n",
    "            weight_linear_rearranged_pruned = np.transpose(weight_linear_rearranged_pruned, (1, 0))\n",
    "            layer.in_features = int(np.shape(weight_linear_rearranged_pruned)[1])\n",
    "            linear_tensor = torch.from_numpy(weight_linear_rearranged_pruned)\n",
    "            layer.weight = torch.nn.Parameter(linear_tensor)\n",
    "\n",
    "            params_linear = np.shape(weight_linear_rearranged_pruned)\n",
    "            C1_1 = params_linear[0]\n",
    "            C2_1 = params_linear[1]\n",
    "\n",
    "            flops_1 = C1_1*C2_1\n",
    "            total_flop_after_pruning +=flops_1\n",
    "\n",
    "    print('flops after pruning:',total_flop_after_pruning)\n",
    "    print(nb_remanining_filters)\n",
    "    net.features = nn.Sequential(*list(net.features.children())[:-1])\n",
    "    print(torch_summarize(net))\n",
    "    \n",
    "    if use_cuda:\n",
    "        net.cuda()\n",
    "        net = torch.nn.DataParallel(net, device_ids=range(torch.cuda.device_count()))\n",
    "        cudnn.benchmark = True\n",
    "\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=args.lr, momentum=0.9, weight_decay=5e-4)\n",
    "    \n",
    "    for epoch in range(start_epoch, start_epoch+args.nbepochs):\n",
    "        train(epoch)\n",
    "        acc_best, num_total, sum_total = test(epoch, num_total, sum_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d77847e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
