{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a2e478",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "from utils import progress_bar\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from torch.nn.modules.module import _addindent\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "import scipy.cluster.hierarchy as hcluster\n",
    "import scipy.cluster.hierarchy as hac\n",
    "import scipy.cluster.hierarchy as fclusterdata\n",
    "import time\n",
    "from sklearn.preprocessing import normalize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75fa7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_summarize(model, show_weights=True, show_parameters=True):\n",
    "    \"\"\"Summarizes torch model by showing trainable parameters and weights.\"\"\"\n",
    "    tmpstr = model.__class__.__name__ + ' (\\n'\n",
    "    total =0\n",
    "    for key, module in model._modules.items():\n",
    "        # if it contains layers let call it recursively to get params and weights\n",
    "        if type(module) in [\n",
    "            torch.nn.modules.container.Container,\n",
    "            torch.nn.modules.container.Sequential\n",
    "        ]:\n",
    "            modstr = torch_summarize(module)\n",
    "        else:\n",
    "            modstr = module.__repr__()\n",
    "        modstr = _addindent(modstr, 2)\n",
    "\n",
    "        params = sum([np.prod(p.size()) for p in module.parameters()])\n",
    "        weights = tuple([tuple(p.size()) for p in module.parameters()])\n",
    "\n",
    "\n",
    "        tmpstr += '  (' + key + '): ' + modstr\n",
    "        if show_weights:\n",
    "            tmpstr += ', weights={}'.format(weights)\n",
    "        if show_parameters:\n",
    "            tmpstr +=  ', parameters={}'.format(params)\n",
    "            total+=params\n",
    "            print(params)\n",
    "            print('total is ',total)\n",
    "        tmpstr += '\\n'\n",
    "\n",
    "    tmpstr = tmpstr + ')'\n",
    "    return tmpstr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f28db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        inputs, targets = Variable(inputs), Variable(targets)\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.data.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets.data).cpu().sum()\n",
    "\n",
    "        progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "            % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe618bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch, num_total, sum_total):\n",
    "    global best_acc\n",
    "    # best_acc = 0.0\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = Variable(inputs, volatile=True), Variable(targets)\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        test_loss += loss.data.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets.data).cpu().sum()\n",
    "\n",
    "        progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "            % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "        num_total+=1\n",
    "        sum_total+=100.*correct/total\n",
    "    \n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        print('Saving weight')\n",
    "        state = {\n",
    "            'net': net.module if use_cuda else net,\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir('checkpoint'):\n",
    "            os.mkdir('checkpoint')\n",
    "        torch.save(state, './checkpoint/ckpt_pruned_retrain.t7')\n",
    "        best_acc = acc\n",
    "\n",
    "    return best_acc, num_total, sum_total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6736a80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_weights_agglo(weight, threshold, average=True,cosine=True,euclidean=False,chebyshev=False,manhattan=False):\n",
    "    t0 = time.time()\n",
    "    weight = weight.T\n",
    "    weight = normalize(weight, norm='l2', axis=1)\n",
    "    threshold =  1.0-threshold   # Conversion to distance measure\n",
    "    if cosine==True:\n",
    "        clusters = hcluster.fclusterdata(weight, threshold, criterion=\"distance\", metric='cosine', depth=1, method='centroid')\n",
    "        z = hac.linkage(weight, metric='cosine', method='complete')\n",
    "    elif euclidean==True:\n",
    "        clusters = hcluster.fclusterdata(weight, threshold, criterion=\"distance\", metric='euclidean', depth=1, method='centroid')\n",
    "        z = hac.linkage(weight, metric='euclidean', method='complete')\n",
    "    elif chebyshev==True:\n",
    "        clusters = hcluster.fclusterdata(weight, threshold, criterion=\"distance\", metric='chebyshev', depth=1, method='centroid')\n",
    "        z = hac.linkage(weight, metric='chebyshev', method='complete')\n",
    "    elif manhattan==True:\n",
    "        clusters = hcluster.fclusterdata(weight, threshold, criterion=\"distance\", metric='cityblock', depth=1, method='centroid')\n",
    "        z = hac.linkage(weight, metric='cityblock', method='complete')\n",
    "    \n",
    "    labels = hac.fcluster(z, threshold, criterion=\"distance\")\n",
    "\n",
    "    labels_unique = np.unique(labels)\n",
    "    n_clusters_ = len(labels_unique)\n",
    "\n",
    "    #print(n_clusters_)\n",
    "    elapsed_time = time.time() - t0\n",
    "    # print(elapsed_time)\n",
    "\n",
    "    a=np.array(labels)\n",
    "    sort_idx = np.argsort(a)\n",
    "    a_sorted = a[sort_idx]\n",
    "    unq_first = np.concatenate(([True], a_sorted[1:] != a_sorted[:-1]))\n",
    "    unq_items = a_sorted[unq_first]\n",
    "    unq_count = np.diff(np.nonzero(unq_first)[0])\n",
    "    unq_idx= np.split(sort_idx, np.cumsum(unq_count))\n",
    "    first_ele = [unq_idx[idx][-1] for idx in range(len(unq_idx))]\n",
    "    return n_clusters_, first_ele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2512f1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc82f94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b8b0db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cbe6ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fbea45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad56ff6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b493c677",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a37288c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
