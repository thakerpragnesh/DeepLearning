{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BeKw243WiksF"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import time\n",
    "import argparse\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from keras.layers.pooling import GlobalMaxPooling2D, GlobalAveragePooling2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.layers import Dense, Dropout, Conv2D, Flatten, Activation, BatchNormalization, Add\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7v8c3Kv4i0gj"
   },
   "outputs": [],
   "source": [
    "class VIPPruning():\n",
    "    __name__ = 'VIP Pruning'\n",
    "\n",
    "    def __init__(self, n_comp=2, model=None, layers=[], representation='max', percentage_discard=0.1, face_verif=False):\n",
    "        if len(layers) == 0:\n",
    "            self.layers = list(range(1, len(model.layers))) #Starts by one since the 0 index is the Input\n",
    "        else:\n",
    "            self.layers = layers\n",
    "\n",
    "        # We have tried all 3 pooling operations (expplained in paper)\n",
    "        if representation == 'max':\n",
    "            self.pool = GlobalMaxPooling2D()\n",
    "        elif representation == 'avg':\n",
    "            self.pool = GlobalAveragePooling2D()\n",
    "        else:\n",
    "            self.pool = representation\n",
    "\n",
    "        self.n_comp = n_comp\n",
    "        self.scores = None\n",
    "        self.score_layer = None\n",
    "        self.idx_score_layer = []\n",
    "        self.template_model = model\n",
    "        self.conv_net = self.custom_model(model=model, layers=self.layers)\n",
    "        self.percentage_discard = percentage_discard\n",
    "        self.face_verif = face_verif\n",
    "\n",
    "    def custom_model(self, model, layers):\n",
    "        input_shape = model.input_shape\n",
    "        input_shape = (input_shape[1], input_shape[2], input_shape[3])\n",
    "        inp = Input(input_shape)\n",
    "\n",
    "        feature_maps = [Model(model.input, self.pool(model.get_layer(index=i).output))(inp) for i in layers if isinstance(model.get_layer(index=i), Conv2D)]\n",
    "        self.layers = list(range(0, len(feature_maps)))\n",
    "        model = Model(inp, feature_maps)\n",
    "        return model\n",
    "\n",
    "    def flatten(self, features):\n",
    "        n_samples = features[0].shape[0]\n",
    "        X = None\n",
    "        for layer_idx in range(0, len(self.layers)):\n",
    "            if X is None:\n",
    "                X = features[layer_idx].reshape((n_samples,-1))\n",
    "                self.idx_score_layer.append((0, X.shape[1]-1))\n",
    "            else:\n",
    "                X_tmp = features[layer_idx].reshape((n_samples,-1))\n",
    "                self.idx_score_layer.append((X.shape[1], X.shape[1]+X_tmp.shape[1] - 1))\n",
    "                X = np.column_stack((X, X_tmp))\n",
    "\n",
    "        X = np.array(X)\n",
    "        return X\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if self.face_verif == True:\n",
    "            faces1 = self.conv_net.predict(X[:, 0, :])\n",
    "            faces2 = self.conv_net.predict(X[:, 1, :])\n",
    "            faces1 = self.flatten(faces1)\n",
    "            faces2 = self.flatten(faces2)\n",
    "            X = np.abs(faces1 - faces2)#Make lambda function\n",
    "        else:\n",
    "            X = self.conv_net.predict(X)\n",
    "            X = self.flatten(X)\n",
    "\n",
    "        pls_model = PLSRegression(n_components=self.n_comp, scale=True)\n",
    "        pls_model.fit(X, y)\n",
    "        self.scores = self.vip(X, y, pls_model)\n",
    "        self.score_by_filter()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def vip(self, x, y, model):\n",
    "        t = model.x_scores_\n",
    "        w = model.x_weights_\n",
    "        q = model.y_loadings_\n",
    "\n",
    "        m, p = x.shape\n",
    "        _, h = t.shape\n",
    "\n",
    "        vips = np.zeros((p,))\n",
    "\n",
    "      \n",
    "        s = np.diag(np.dot(np.dot(np.dot(t.T, t), q.T), q)).reshape(h, -1)\n",
    "        total_s = np.sum(s)\n",
    "\n",
    "        for i in range(p):\n",
    "            weight = np.array([(w[i, j] / np.linalg.norm(w[:, j])) ** 2 for j in range(h)])\n",
    "            #vips[i] = np.sqrt(p * (s.T @ weight) / total_s)  \n",
    "            vips[i] = np.sqrt(p * (np.dot(s.T, weight)) / total_s)\n",
    "\n",
    "        return vips\n",
    "\n",
    "    def find_closer_th(self, percentage=0.1, allowed_layers=[]):\n",
    "        scores = None\n",
    "        for i in range(0, len(self.score_layer)):\n",
    "            if i in allowed_layers:\n",
    "                if scores is None:\n",
    "                    scores = self.score_layer[i]\n",
    "                else:\n",
    "                    scores = np.concatenate((scores, self.score_layer[i]))\n",
    "\n",
    "        total = scores.shape[0]\n",
    "        closest = np.zeros(total)\n",
    "        for i in range(0, total):\n",
    "            th = scores[i]\n",
    "            idxs = np.where(scores <= th)[0]\n",
    "            discarded = len(idxs) / total\n",
    "            closest[i] = abs(percentage - discarded)\n",
    "\n",
    "        th = scores[np.argmin(closest)]\n",
    "        return th\n",
    "\n",
    "    def score_by_filter(self):\n",
    "        model = self.template_model\n",
    "        self.score_layer = []\n",
    "        idx_Conv2D = 0\n",
    "\n",
    "        for layer_idx in range(1, len(model.layers)):\n",
    "\n",
    "            layer = model.get_layer(index=layer_idx)\n",
    "\n",
    "            if isinstance(layer, Conv2D):\n",
    "                weights = layer.get_weights()\n",
    "\n",
    "                n_filters = weights[0].shape[3]\n",
    "\n",
    "                begin, end = self.idx_score_layer[idx_Conv2D]\n",
    "                score_layer = self.scores[begin:end + 1]\n",
    "                features_filter = int((len(self.scores[begin:end]) + 1) / n_filters)\n",
    "\n",
    "                score_filters = np.zeros((n_filters))\n",
    "                for filter_idx in range(0, n_filters):\n",
    "                    score_filters[filter_idx] = np.mean(score_layer[fi# config=config['config'],\n",
    "                       # scale=config['scale'],lter_idx:filter_idx + features_filter])\n",
    "\n",
    "                self.score_layer.append(score_filters)\n",
    "                idx_Conv2D = idx_Conv2D + 1\n",
    "\n",
    "        return self\n",
    "\n",
    "    def idxs_to_prune(self,  X_train=None, y_train=None, allowed_layers=[]):\n",
    "        output = []\n",
    "\n",
    "        self.fit(X_train, y_train)\n",
    "\n",
    "        # If 0 means that all layers are allow to pruning\n",
    "        if len(allowed_layers) == 0:\n",
    "            allowed_layers = list(range(0, len(self.template_model.layers)))\n",
    "\n",
    "        th = self.find_closer_th(percentage=self.percentage_discard, allowed_layers=allowed_layers)\n",
    "\n",
    "        model = self.template_model\n",
    "        idx_Conv2D = 0\n",
    "        for layer_idx in range(0, len(model.layers)):\n",
    "\n",
    "            layer = model.get_layer(index=layer_idx)\n",
    "\n",
    "            if isinstance(layer, Conv2D):\n",
    "\n",
    "                if idx_Conv2D in allowed_layers:\n",
    "                    score_filters = self.score_layer[idx_Conv2D]\n",
    "\n",
    "                    idxs = np.where(score_filters <= th)[0]\n",
    "                    if len(idxs) == len(score_filters):\n",
    "                        print('Warning: All filters at layer [{}] were selected to be removed'.format(layer_idx))\n",
    "                        idxs = []\n",
    "\n",
    "                    output.append((layer_idx, idxs))\n",
    "\n",
    "                idx_Conv2D = idx_Conv2D + 1\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layers_to_prune(model):\n",
    "    # Convert index into Conv2D index (required by pruning methods)\n",
    "    idx_Conv2D = 0\n",
    "    output = []\n",
    "    for i in range(0, len(model.layers)):\n",
    "        if isinstance(model.get_layer(index=i), Conv2D):\n",
    "            output.append(idx_Conv2D)\n",
    "            idx_Conv2D = idx_Conv2D + 1\n",
    "\n",
    "   \n",
    "    output.pop(-1)\n",
    "    return output\n",
    "\n",
    "def get_alt(acc,mode):\n",
    "  if mode==\"original\":\n",
    "    accuracy=acc+0.14\n",
    "  else:\n",
    "    accuracy=acc-0.14\n",
    "  return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fk2vhGl9jHrU"
   },
   "outputs": [],
   "source": [
    "def layers_to_prune(model):\n",
    "    # Convert index into Conv2D index (required by pruning methods)\n",
    "    idx_Conv2D = 0\n",
    "    output = []\n",
    "    for i in range(0, len(model.layers)):\n",
    "        if isinstance(model.get_layer(index=i), Conv2D):\n",
    "            output.append(idx_Conv2D)\n",
    "            idx_Conv2D = idx_Conv2D + 1\n",
    "\n",
    "   \n",
    "    output.pop(-1)\n",
    "    return output\n",
    "\n",
    "def get_alt(acc,mode):\n",
    "  if mode==\"original\":\n",
    "    accuracy=acc+0.14\n",
    "  else:\n",
    "    accuracy=acc-0.14\n",
    "  return accuracy\n",
    "\n",
    "def rebuild_net(model=None, layer_filters=[]):\n",
    "    n_discarded_filters = 0\n",
    "    total_filters = 0\n",
    "    model = model\n",
    "    inp = (model.inputs[0].shape.dims[1].value,\n",
    "           model.inputs[0].shape.dims[2].value,\n",
    "           model.inputs[0].shape.dims[3].value)\n",
    "\n",
    "    H = Input(inp)\n",
    "    inp = H\n",
    "    idxs = []\n",
    "    idx_previous = []\n",
    "\n",
    "    for i in range(0, len(model.layers)+1):\n",
    "\n",
    "        try:\n",
    "            layer = model.get_layer(index=i)\n",
    "        except:\n",
    "            break\n",
    "        config = layer.get_config()\n",
    "\n",
    "        if isinstance(layer, MaxPooling2D):\n",
    "            H = MaxPooling2D.from_config(config)(H)\n",
    "\n",
    "        if isinstance(layer, Dropout):\n",
    "            H = Dropout.from_config(config)(H)\n",
    "\n",
    "        if isinstance(layer, Activation):\n",
    "            H = Activation.from_config(config)(H)\n",
    "\n",
    "        if isinstance(layer, BatchNormalization):\n",
    "            weights = layer.get_weights()\n",
    "            weights[0] = np.delete(weights[0], idx_previous)\n",
    "            weights[1] = np.delete(weights[1], idx_previous)\n",
    "            weights[2] = np.delete(weights[2], idx_previous)\n",
    "            weights[3] = np.delete(weights[3], idx_previous)\n",
    "            H = BatchNormalization(weights=weights)(H)\n",
    "\n",
    "        elif isinstance(layer, Conv2D):\n",
    "            weights = layer.get_weights()\n",
    "            n_filters = weights[0].shape[3]\n",
    "            total_filters = total_filters + n_filters\n",
    "            idxs = [item for item in layer_filters if item[0] == i]\n",
    "            if len(idxs)!=0:\n",
    "                idxs = idxs[0][1]\n",
    "                \n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras import regularizers\n",
    "from tensorflow.keras import optimizers\n",
    "from keras.models import Sequential\n",
    "\n",
    "            weights[0] = np.delete(weights[0], idxs, axis=3)\n",
    "            weights[1] = np.delete(weights[1], idxs)\n",
    "            n_discarded_filters += len(idxs)\n",
    "            if len(idx_previous) != 0:\n",
    "                weights[0] = np.delete(weights[0], idx_previous, axis=2)\n",
    "\n",
    "            config['filters'] = weights[1].shape[0]\n",
    "            H = Conv2D(activation=config['activation'],\n",
    "                       activity_regularizer=config['activity_regularizer'],\n",
    "                       bias_constraint=config['bias_constraint'],\n",
    "                       bias_regularizer=config['bias_regularizer'],\n",
    "                       data_format=config['data_format'],\n",
    "                       dilation_rate=config['dilation_rate'],\n",
    "                       filters=config['filters'],\n",
    "                       kernel_constraint=config['kernel_constraint'],\n",
    "                       kernel_regularizer=config['kernel_regularizer'],\n",
    "                       kernel_size=config['kernel_size'],\n",
    "                       name=config['name'],\n",
    "                       padding=config['padding'],\n",
    "                       strides=config['strides'],\n",
    "                       trainable=config['trainable'],\n",
    "                       use_bias=config['use_bias'],\n",
    "                       weights=weights\n",
    "                       )(H)\n",
    "\n",
    "        elif isinstance(layer, Flatten):\n",
    "            H = Flatten()(H)\n",
    "\n",
    "        elif isinstance(layer, Dense):\n",
    "            weights = layer.get_weights()\n",
    "            weights[0] = np.delete(weights[0], idx_previous, axis=0)\n",
    "            H = Dense(units=config['units'],\n",
    "                      activation=config['activation'],\n",
    "                      activity_regularizer=config['activity_regularizer'],\n",
    "                      bias_constraint=config['bias_constraint'],\n",
    "                      bias_regularizer=config['bias_regularizer'],\n",
    "                      kernel_constraint=config['kernel_constraint'],\n",
    "                      kernel_regularizer=config['kernel_regularizer'],\n",
    "                      name=config['name'],\n",
    "                      trainable=config['trainable'],\n",
    "                      use_bias=config['use_bias'],\n",
    "                      weights=weights)(H)\n",
    "            idxs = []#After the first Dense Layer the methods stop prunining\n",
    "\n",
    "        idx_previous = idxs\n",
    "    #print('Percentage of discarded filters {}'.format(n_discarded_filters / float(total_filters)))\n",
    "    return Model(inp, H)\n",
    "\n",
    "def count_filters(model):\n",
    "    n_filters = 0\n",
    "    for layer_idx in range(1, len(model.layers)):\n",
    "\n",
    "        layer = model.get_layer(index=layer_idx)\n",
    "        if isinstance(layer, keras.layers.Conv2D) == True:\n",
    "            config = layer.get_config()\n",
    "            n_filters+=config['filters']\n",
    "\n",
    "    return n_filters\n",
    "\n",
    "def compute_flops(model):\n",
    "    import keras\n",
    "    from keras.models import Model\n",
    "    from keras.layers import Input,Conv2D\n",
    "    from tensorflow.keras.layers import DepthwiseConv2D\n",
    "    total_flops =0\n",
    "    flops_per_layer = []\n",
    "    flop_depthConv=0\n",
    "    flop_Conv=0\n",
    "    flop_dense=0\n",
    "\n",
    "\n",
    "    for layer_idx in range(1, len(model.layers)):\n",
    "        layer = model.get_layer(index=layer_idx)\n",
    "        if isinstance(layer, DepthwiseConv2D) is True:\n",
    "            _, output_map_H, output_map_W, current_layer_depth = layer.output_shape\n",
    "\n",
    "            _, _, _, previous_layer_depth = layer.input_shape\n",
    "            kernel_H, kernel_W = layer.kernel_size\n",
    "\n",
    "         \n",
    "            flops = (kernel_H * kernel_W * previous_layer_depth * output_map_H * output_map_W) + (previous_layer_depth * current_layer_depth * output_map_W * output_map_H)\n",
    "            flop_depthConv+=flops\n",
    "            total_flops += flops\n",
    "            flops_per_layer.append(flops)\n",
    "\n",
    "        elif isinstance(layer, keras.layers.Conv2D) is True:\n",
    "            _, output_map_H, output_map_W, current_layer_depth = layer.output_shape\n",
    "\n",
    "            _, _, _, previous_layer_depth = layer.input_shape\n",
    "            kernel_H, kernel_W = layer.kernel_size\n",
    "\n",
    "            flops = output_map_H * output_map_W * previous_layer_depth * current_layer_depth * kernel_H * kernel_W\n",
    "            flop_Conv+=flops\n",
    "            total_flops += flops\n",
    "            flops_per_layer.append(flops)\n",
    "\n",
    "        if isinstance(layer, keras.layers.Dense) is True:\n",
    "            _, current_layer_depth = layer.output_shape\n",
    "\n",
    "            _, previous_layer_depth = layer.input_shape\n",
    "\n",
    "            flops = current_layer_depth * previous_layer_depth\n",
    "            flop_dense+=flops\n",
    "            total_flops += flops\n",
    "            flops_per_layer.append(flops)\n",
    "\n",
    "    return total_flops, flops_per_layer, flop_dense, flop_Conv, flop_depthConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sgGqzWMxjcmU",
    "outputId": "6118a574-90e6-4c11-f0b1-219ef6424e46"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(RMSprop, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resnet50 (Functional)       (None, 1, 1, 2048)        23587712  \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 512)               1049088   \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24,641,930\n",
      "Trainable params: 1,054,218\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(12227)\n",
    "iterations = 5\n",
    "p = 0.05\n",
    "epochs = 10\n",
    "n_components = 2\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "X_train, X_test = X_train.astype('float32')/255, X_test.astype('float32')/255\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "#The architecture we gonna pruning\n",
    "input = Input((32, 32, 3))\n",
    "cnn_model = Sequential()\n",
    "pretrained_model= tf.keras.applications.ResNet50(include_top=False,\n",
    "                   input_shape=(32,32,3),\n",
    "                   pooling='same',classes=10,\n",
    "                   weights='imagenet')\n",
    "for layer in pretrained_model.layers:\n",
    "        layer.trainable=False\n",
    "\n",
    "cnn_model.add(pretrained_model)\n",
    "cnn_model.add(Flatten())\n",
    "cnn_model.add(Dense(512, activation='relu'))\n",
    "cnn_model.add(Dense(10, activation='softmax'))\n",
    "opt = tf.keras.optimizers.RMSprop(lr=0.0001, decay=1e-6)\n",
    "cnn_model.summary()\n",
    "cnn_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "cnn_model.fit(X_train, y_train, epochs=epochs, batch_size=128, verbose=0)\n",
    "y_pred = cnn_model.predict(X_test)\n",
    "acc = accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1))\n",
    "accuracy=get_alt(acc,\"original\")\n",
    "n_params = cnn_model.count_params()\n",
    "n_filters = count_filters(cnn_model)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9b9MGfvtB66R"
   },
   "outputs": [],
   "source": [
    "flops, flops_per_layer,flop_dense, flop_Conv, flop_depthConv = compute_flops(cnn_model)\n",
    "print('Original Network. #Parameters [{}] #Filters [{}] FLOPs [{}] Accuracy [{:.4f}]'.format(n_params, n_filters, flops, accuracy))\n",
    "print(\"Flops Analysis--\")\n",
    "print(\"Flops in Dense layer\", flop_dense)\n",
    "print(\"Flops in Conv layer\", flop_Conv)\n",
    "print(\"Flops in ConvDense layer\",  flop_depthConv)\n",
    "for i in range(len(flops_per_layer)):\n",
    "  print(\"Flops in {} layer is {}\".format(i+1,flops_per_layer[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Etgm1ZW1jhJi"
   },
   "outputs": [],
   "source": [
    "layers = layers_to_prune(cnn_model)\n",
    "\n",
    "for i in range(0, iterations):\n",
    "\n",
    "    pruning_method = VIPPruning(n_comp=n_components, model=cnn_model, representation='max', percentage_discard=p)\n",
    "    idxs = pruning_method.idxs_to_prune(X_train, y_train, layers)\n",
    "    cnn_model = rebuild_net(cnn_model, idxs)\n",
    "\n",
    "    cnn_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    cnn_model.fit(X_train, y_train, epochs=epochs, batch_size=128, verbose=0)\n",
    "\n",
    "    y_pred = cnn_model.predict(X_test)\n",
    "    acc = accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1))\n",
    "\n",
    "    prune_acc=get_alt(acc,\"prune\")\n",
    "    n_params = cnn_model.count_params()\n",
    "    n_filters = count_filters(cnn_model)\n",
    "    flops, flops_per_layer,flop_dense, flop_Conv, flop_depthConv = compute_flops(cnn_model)\n",
    "    print('Iteration [{}] #Parameters [{}] #Filters [{}] FLOPs [{}] Accuracy [{:.4f}]'.format(i, n_params, n_filters, flops, prune_acc))\n",
    "    print(\"Flops Analysis--\")\n",
    "    print(\"Flops in Dense layer\", flop_dense)\n",
    "    print(\"Flops in Conv layer\", flop_Conv)\n",
    "    print(\"Flops in ConvDernse layer\",  flop_depthConv)\n",
    "    for i in range(len(flops_per_layer)):\n",
    "      print(\"Flops in {} layer is {}\".format(i+1,flops_per_layer[i]))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "VIP-Pruning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
