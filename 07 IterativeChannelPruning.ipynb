{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "'''\n",
    "import os\n",
    "def ensure_dir(dir_path):\n",
    "    directory = os.path.dirname(dir_path)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Library for file operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\"\"\"\n",
    "Following Cell facilitate to read files from drive and and help to \n",
    "read dataset\n",
    "\"\"\"\n",
    "# import library to perform file operation\n",
    "import os #use to access the files \n",
    "import tarfile # use to extract dataset from zip files\n",
    "import sys\n",
    "import zipfile\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Torch Library provides facilities to create networl architechture and write farword and backwor phase od neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import torch library to build neural network\n",
    "import torch  # Elementory function of tensor is define in torch package\n",
    "import torch.nn as nn # Several layer architectur is define here\n",
    "import torch.nn.functional as F # loss function and activation function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computer vision is one of the most important application and thus lots of deplopment in the and torch.vision provides many facilities that can be use to imporve model such as data augmentation, reading data batchwise, suffling data before each epoch and many more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch library related to image data processing\n",
    "import torchvision # provides facilities to access image dataset\n",
    "from torchvision.datasets.utils import download_url \n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import random_split\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision import datasets, models, transforms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set input directories path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DatasetLoc = '/home3/pragnesh/Dataset/'\n",
    "SelDataSet = 'IntelIC'\n",
    "trainDir = 'train'\n",
    "testDir = 'test'\n",
    "data_dir = DatasetLoc+SelDataSet\n",
    "zipFile = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select output directories path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SelectOutLoc = DatasetLoc+\"Intel_Image_Classifacation_v2/\"  #/home3/pragnesh/Dataset/Intel_Image_Classifacation_v2/\n",
    "LogLoc =   SelectOutLoc+\"Logs/\"\n",
    "outfile = LogLoc+\"FinalOutv2.log\"\n",
    "logFile = LogLoc+\"ConvModelv2.log\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the pretrain model is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### Model #################################\n",
    "ifLoadModel = False\n",
    "modelname = 'vgg11' # vgg11 vgg13 vgg16,  resnet18,  savedmodel\n",
    "ifTransferLearning = True\n",
    "NumberOfClass = 6\n",
    "ModelLoc = SelectOutLoc+\"Model/\"\n",
    "SavePath = SelectOutLoc+'Model/VGG_IntelIC_v1-'+modelname\n",
    "LoadPath = SelectOutLoc+'Model/VGG_IntelIC_v1-'+modelname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set all the require hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### Hyper Parameter #########################\n",
    "\"\"\"\n",
    "***************************************************************\n",
    "We are using one cycle fit function in which learning rate start with 1/10th \n",
    "of selected maximum learning rate and increase learning rate from min to max\n",
    "in 1st phase and then decrease from max to min in 2nd phase\n",
    "***************************************************************\n",
    "Set all the Hyper Parameter Such as \n",
    "1. Learning Rate to control step size\n",
    "2. grad_clip to control the maximum value of gradient\n",
    "3. weight decay to control L2 regularization\n",
    "4. L1 to control L1 regularization\n",
    "5. opt_func to select optimization function\n",
    "***************************************************************\n",
    "\"\"\"\n",
    "max_lr = 1e-3\n",
    "epochs = 30\n",
    "grad_clip = 0.2 \n",
    "weight_decay = 1e-4 \n",
    "L1 = 1e-5\n",
    "opt_func = torch.optim.Adam\n",
    "MODEL_NAME = f\"VGG_Net-{modelname}\\t MLR-{max_lr}-GC{grad_clip}-WD-{weight_decay}-L1-{L1}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create output directories if not exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "ensure_dir(SelectOutLoc)\n",
    "ensure_dir(ModelLoc)\n",
    "ensure_dir(LogLoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Based on the image size of the dataset choose apropriate values of the color channel and Image Size \n",
    "#### Here we can define path to a folder where we can keep all the dataset. \n",
    "#### In the following we are using the zip files. \n",
    "#### Originally dataset should be in the following format DataSetName is parent folder and it should contain train and test folder. train and test folder should contain folder for each category and images of respective category should be in the respective category folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#Data Prepration 1\n",
    "\n",
    "\"\"\"\n",
    "######################### Data Loading #########################################\n",
    "if zipFile == True:\n",
    "  fullpath = DatasetLoc+SelDataSet+'.zip'\n",
    "  zip_ref = zipfile.ZipFile(fullpath, 'r') #Opens the zip file in read mode\n",
    "  zip_ref.extractall('/tmp') #Extracts the files into the /tmp folder\n",
    "  zip_ref.close()\n",
    "  data_dir = \"/tmp/IntelIC/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create batches and perform tranformation over it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Create Batch Of Dataset and do data augmentation ###########\n",
    "bs = 128\n",
    "ImageSize = 224\n",
    "\n",
    "\"\"\"\n",
    "Data Augmentaion generally help in reducing overfitting error during \n",
    "trainng process and thus we are performing randon horizontal flip and \n",
    "random crop during training but during validation as no training happens \n",
    "we dont perform data augmentation\n",
    "\"\"\"\n",
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "data_transforms = {\n",
    "    trainDir: transforms.Compose([\n",
    "        transforms.RandomResizedCrop(ImageSize),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    testDir: transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in [trainDir, testDir]}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=bs,\n",
    "                                             shuffle=True, num_workers=1)\n",
    "              for x in [trainDir, testDir]}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in [trainDir, testDir]}\n",
    "class_names = image_datasets[trainDir].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\"\"\"Check if Cuda GPU is available\"\"\"\n",
    "#check for CUDA enabled GPU card\n",
    "def getDeviceType():\n",
    "  if torch.cuda.is_available():\n",
    "    return torch.device('cuda')\n",
    "  else:\n",
    "    return torch.device('cpu')\n",
    "device = getDeviceType()\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define function to facilitate training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#Training Process\n",
    "Below code will work as a base function and provide all the important \n",
    "function like compute loss, accuracy and print result in a perticular \n",
    "formate afte each epoch. Funvtion are as follow\n",
    "1. Accuracy : Computer accuracy in evalutaion mode of pytorch on given dataset for given model\n",
    "2. compute_batch_loss : Compute batch loss and append the loss in the list of batch loss.\n",
    "3. compute_batch_loss_acc : Compute batch loss, batch accuracy and append the loss in the list of batch loss.\n",
    "4. accumulate_batch_loss_acc: Accumulate loss from the list of batch and acccuraly loss.\n",
    "5. Epoch end to print the output after every epoch in proper format\n",
    "\"\"\"\n",
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1) \t\t# get the prediction vector\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "# Compute loss of the given batch and return it\n",
    "def compute_batch_loss(newmodel, batch_X,batch_y):\n",
    "  images = batch_X.to(device)\n",
    "  labels = batch_y.to(device)\n",
    "  out = newmodel(images)                  \t\t# Generate predictions\n",
    "  loss = F.cross_entropy(out, labels) \t\t\t# Calculate loss\n",
    "  return loss\n",
    "\n",
    "# Computes loss and accuracy of the given batch(Used in validation)\n",
    "def compute_batch_loss_acc(newmodel, batch_X,batch_y):\n",
    "    images = batch_X.to(device)\n",
    "    labels = batch_y.to(device)\n",
    "    out = newmodel(images)                    \t# Generate predictionsin_features=4096\n",
    "    loss = F.cross_entropy(out, labels)   \t\t# Calculate loss\n",
    "    acc = accuracy(out, labels)           \t\t# Calculate accuracy\n",
    "    return {'val_loss': loss, 'val_acc': acc}\n",
    "\n",
    "# At the end of epoch accumulate all batch loss and batch accueacy    \n",
    "def accumulate_batch_loss_acc(outputs):\n",
    "    batch_losses = [x['val_loss'] for x in outputs]\n",
    "    epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "    batch_accs = [x['val_acc'] for x in outputs]\n",
    "    epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "    return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "\n",
    "def epoch_end(epoch, result):\n",
    "  # Print in given format \n",
    "  # Epoch [0], last_lr: 0.00278, train_loss: 1.2862, val_loss: 1.2110, val_acc: 0.6135\n",
    "  strResult = \"Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "      epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_acc'])\n",
    "  print(strResult)\n",
    "  return strResult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define training function and select optimization function and learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"## Define Training \n",
    "Here we will evalute our model after each epoch on validation dataset using evalute method\n",
    "get_lr method returnd last learning rate used in the training\n",
    "Here we are using one fit cycle method in which we specify the max learning rate and learning \n",
    "rate start from 1/10th value of max_lr and slowly increases the value to max_lr for 40% of updates \n",
    "then decreases to its initial value for 40% updates and then further decreases to 1/100th of max_lr \n",
    "value to perform final fine tuning.\n",
    "\"\"\"\n",
    "# evalute model on given dataset using given data loader\n",
    "@torch.no_grad()\n",
    "# evalute model on given dataset using given data loader\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      for batch_X, batch_y in data_loader:\n",
    "        outputs = [compute_batch_loss_acc(model,batch_X,batch_y)]\n",
    "      return accumulate_batch_loss_acc(outputs)\n",
    "\n",
    "# Use special scheduler to change the value of learning rate\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "# epoch=8, max_lr=.01, weight_decay(L2-Regu parametr)=.0001,opt_func=Adam\n",
    "\n",
    "######### Main Function To Implement Training #################\n",
    "def fit_one_cycle(ModelName,epochs, max_lr, model, \n",
    "                  weight_decay=0, L1=0,grad_clip=None, opt_func=torch.optim.SGD):\n",
    "    torch.cuda.empty_cache()\n",
    "    history = []\n",
    "    # Set up cutom optimizer here we will use one cycle scheduler with max learning\n",
    "    # rate given by max_lr, default optimizer is SGD but we will use ADAM, and \n",
    "    # L2 Regularization using weight decay\n",
    "    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n",
    "    # Set up one-cycle learning rate scheduler\n",
    "    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, \n",
    "                                                steps_per_epoch=len(dataloaders[trainDir]))\n",
    "    print(\"Training Starts\")\n",
    "    with open(logFile, \"a\") as f:\n",
    "      for epoch in range(epochs):\n",
    "          # Training Phase \n",
    "          model.train()  #######################\n",
    "          train_losses = []\n",
    "          lrs = []\n",
    "          #for batch in train_loader:\n",
    "          for batch_X, batch_y in dataloaders[trainDir]:\n",
    "              # computer the training loss of current batch\n",
    "              loss = compute_batch_loss(model,batch_X,batch_y)\n",
    "              l1_crit = nn.L1Loss()\n",
    "              reg_loss = 0\n",
    "              for param in model.parameters():\n",
    "                reg_loss += l1_crit(param,target=torch.zeros_like(param))\n",
    "              loss += L1*reg_loss \n",
    "              \n",
    "              train_losses.append(loss)\n",
    "              loss.backward() # compute the gradient of all weights\n",
    "              # Clip the gradient value to maximum allowed grad_clip value\n",
    "              if grad_clip: \n",
    "                  nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
    "              optimizer.step() # Updates weights \n",
    "              # pytorch by default accumulate grade history and if we dont want it\n",
    "              # we should make all previous grade value equals to zero\n",
    "              optimizer.zero_grad() \n",
    "              # Record & update learning rate\n",
    "              lrs.append(get_lr(optimizer))\n",
    "              sched.step() # Update the learning rate\n",
    "              # Compute Validation Loss and Valodation Accuracy\n",
    "              result = evaluate(model, dataloaders[testDir])\n",
    "              # Compute Train Loss of whole epoch i.e mean of loss of batch \n",
    "              result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "              # Observe how learning rate is change by schedular\n",
    "              result['lrs'] = lrs\n",
    "              # print the observation of each epoch in a proper format\n",
    "          \n",
    "          #strResult = \"Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, \n",
    "            #val_acc: {:.4f}\".format(epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_acc'])\n",
    "          strResult = epoch_end(epoch, result) \n",
    "          \n",
    "          f.write(f\"{ModelName}-\\t{strResult}\\n\")\n",
    "          print(strResult)\n",
    "          history.append(result) # append tupple result with val_acc, vall_loss, and trin_loss\n",
    "        \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the appropriate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[11]: Load the apropiate model for training\n",
    "############################# Use Pre-Train Model for Transfer Learning #####################################\n",
    "#################### Download Pretrain VGGNet ################\n",
    "if ifLoadModel == False:\n",
    "    if modelname == 'vgg16' or modelname == 'vgg13' or modelname == 'vgg11':\n",
    "        if modelname == 'vgg11':\n",
    "            newModel = torchvision.models.vgg11(pretrained=ifTransferLearning)\n",
    "        if modelname == 'vgg13':\n",
    "            newModel = torchvision.models.vgg13(pretrained=ifTransferLearning)\n",
    "        if modelname == 'vgg16':\n",
    "            newModel = torchvision.models.vgg16(pretrained=ifTransferLearning)    \n",
    "        for param in newModel.parameters():\n",
    "            param.requires_grad = False\n",
    "        #Need to change the below code if we choose different model\n",
    "        print(newModel.classifier[6])\n",
    "        num_ftrs = newModel.classifier[6].in_features\n",
    "        # Here the size of each output sample is set to 10.\n",
    "        # Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
    "        newModel.classifier[6] = nn.Linear(num_ftrs, NumberOfClass)\n",
    "        newModel = newModel.to(device)\n",
    "\n",
    "    ##################### Download Pretrain ResNet 18 ############################\n",
    "    if modelname == 'resnet18':\n",
    "        newModel = torchvision.models.resnet18(pretrained=False)\n",
    "        print(newModel.fc)\n",
    "        for param in newModel.parameters():\n",
    "            param.requires_grad = False\n",
    "        #print(newModel)\n",
    "        #Need to change the below code if we choose different model\n",
    "        num_ftrs = newModel.fc.in_features\n",
    "        # Here the size of each output sample is set to 10.\n",
    "        # Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
    "        newModel.fc = nn.Linear(num_ftrs, 10)\n",
    "        newModel = newModel.to(device)\n",
    "################# Load Stored Trained Model #####################################\n",
    "if ifLoadModel == True:\n",
    "    if device== torch.device('cpu'):\n",
    "        newModel = torch.load(LoadPath, map_location=torch.device('cpu'))\n",
    "    else:\n",
    "        newModel = torch.load(LoadPath, map_location=torch.device('cuda'))\n",
    "    # Need to change the following loop for dofferent model\n",
    "    count=0\n",
    "    if modelname == 'VGG16':\n",
    "        for param in newModel.parameters():\n",
    "            if count in (26,28,30):\n",
    "              param.requires_grad=True\n",
    "            else:\n",
    "              param.requires_grad=False\n",
    "            \n",
    "    \n",
    "    if modelname == 'VGG13':\n",
    "        for param in newModel.parameters():\n",
    "            if count in (20,22,24):\n",
    "              param.requires_grad=True\n",
    "            else:\n",
    "              param.requires_grad=False\n",
    "            \n",
    "    if modelname == 'VGG11':\n",
    "        for param in newModel.parameters():\n",
    "            if count in (16,18,20):\n",
    "              param.requires_grad=True\n",
    "            else:\n",
    "              param.requires_grad=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[13]: Train the Model\n",
    "\"\"\"\n",
    "Start the training using one fit cycle algorithms where learning rate start with one tenth of provided max_lr and then increase it value till that point and then decrease onword and for last few epoch learning rate furthe decrease\n",
    "Pass the hyperparameter for training of the model and start training process for set number of epcoh\n",
    "\"\"\"\n",
    "# Commented out IPython magic to ensure Python compatibility.\n",
    "# %%time\n",
    "historylast=[]\n",
    "historylast += fit_one_cycle(MODEL_NAME,epochs, max_lr, newModel,  \n",
    "                              grad_clip=grad_clip, \n",
    "                              weight_decay=weight_decay, L1=L1\n",
    "                              ,opt_func=opt_func\n",
    "                              )\n",
    "########################################### Evalute the Training Process by Plotting Graph ####################################################\n",
    "with open(outfile,'a') as f:\n",
    "    f.write(\"Training End Here\")\n",
    "torch.save(newModel, SavePath)\n",
    "accuracies = [x['val_acc']*100 for x in historylast]\n",
    "accuracies = [\"%.2f\" % v for v in accuracies]\n",
    "print(accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define accuracy to evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"##Define method to compute accuracy for a given model on given dataset\"\"\"\n",
    "def accuraciesTotal(newModel, data_loader):\n",
    "  with torch.no_grad():\n",
    "    acc = []\n",
    "    for batch in data_loader:\n",
    "        images, label = batch\n",
    "        images, labels = batch[0].to(device), batch[1].to(device)\n",
    "        out = newModel(images)\n",
    "        acc.append(accuracy(out, labels))\n",
    "    return torch.mean(torch.stack(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare train and test dataet to evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"###Prepare the data loader for inference. During the inference \n",
    "we want to perform same tranformation on test and train dataset\"\"\"\n",
    "image_datasets_eval = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[testDir])\n",
    "                  for x in [trainDir, testDir]}\n",
    "dataloaders_eval = {x: torch.utils.data.DataLoader(image_datasets_eval[x], batch_size=bs,\n",
    "                                             shuffle=False, num_workers=1)\n",
    "                  for x in [trainDir, testDir]}\n",
    "\n",
    "# Evalutute train and test error\n",
    "trainacc = accuraciesTotal(newModel,dataloaders_eval[trainDir])\n",
    "testacc = accuraciesTotal(newModel,dataloaders_eval[testDir])\n",
    "with open(outfile,'a') as f:\n",
    "    f.write(f\"Train Accuracy :  {trainacc}\\n Test Accuracy  :  {testacc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create prunelist for vggnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[18]:\n",
    "import torch.nn.utils.prune as prune\n",
    "import time\n",
    "\n",
    "###Select the features want to prune and store them in the list of of Mudule here\"\"\"\n",
    "#print(newModel.features)\n",
    "Module = []\n",
    "if modelname == 'vgg11':\n",
    "    prunelist = [0, 3, 6,8, 11,13, 16,18]\n",
    "if modelname == 'vgg13':\n",
    "    prunelist = [0,2, 5,7, 10,12, 15,17, 20,22]\n",
    "if modelname == 'vgg16':\n",
    "    prunelist = [0,2, 5,7, 10,12,14, 17,19,21, 24,26,28]\n",
    "\n",
    "pr = [] \n",
    "maxpr=.05\n",
    "length = len(prunelist)\n",
    "print(length)\n",
    "count=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### perform Ln structure pruning itteratively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Select the amount of feature we want to prune in each Layer\"\"\"\n",
    "epochs = 10\n",
    "max_lr = 1e-3\n",
    "grad_clip = .2 \n",
    "weight_decay = 1e-5 \n",
    "L1=1e-5\n",
    "itteration = 6\n",
    "for  ittr in range(itteration):\n",
    "    for i in range(len(Module)):\n",
    "        prune.ln_structured(Module[i], name=\"weight\", amount=pr[i], n=1, dim=0)\n",
    "    numberOfZero = 0\n",
    "    numberOfElements = 0\n",
    "    totalNumberOfZero=0\n",
    "    totalNumberOfElements=0\n",
    "    for i, j in zip(range(len(prunelist)),prunelist):\n",
    "        numberOfZero = torch.sum(Module[i].weight == 0)\n",
    "        totalNumberOfZero += numberOfZero\n",
    "        numberOfElements = Module[i].weight.nelement()\n",
    "        totalNumberOfElements += numberOfElements\n",
    "        frac = 100. * float(torch.sum(Module[i].weight == 0))/float(Module[i].weight.nelement())\n",
    "        with open(outfile,'a') as f:\n",
    "            f.write((f\"{j} Sparsity in {Module[i]} is \\t{frac}\"))\n",
    "        \n",
    "    with open(outfile,'a') as f:\n",
    "        f.write((\"Global Sparsity: {:.2f}%\".format(100*float(numberOfZero)/float(numberOfElements))))\n",
    "\n",
    "    print(f\"******************** {ittr}**********************\")\n",
    "    #fit_one_cycle(ModelName,epochs, max_lr, model,weight_decay=0, L1=0,grad_clip=None, opt_func=torch.optim.SGD):\n",
    "    historyPrun = fit_one_cycle(MODEL_NAME,epochs, max_lr, newModel,  \n",
    "                              grad_clip=grad_clip, \n",
    "                              weight_decay=weight_decay, L1=L1,\n",
    "                              opt_func=opt_func\n",
    "                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs =30\n",
    "historyPrun = fit_one_cycle(MODEL_NAME,epochs, max_lr, newModel,\n",
    "              grad_clip=grad_clip,\n",
    "              weight_decay=weight_decay, L1=L1,\n",
    "              opt_func=opt_func\n",
    "              )\n",
    "\n",
    "for i,j in zip(prunelist,range(len(prunelist))):\n",
    "    prune.remove(newModel.features[i], 'weight')\n",
    "\n",
    "savePrunePath = SavePath+\"-prune\"\n",
    "torch.save(newModel, savePrunePath)\n",
    "\n",
    "with open(outfile,'a') as f:\n",
    "    f.write(\"execution completed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
