{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thakerpragnesh/DeepLearning/blob/master/06_VGG16TranferLearningPruning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVjg8mhW6uHs"
      },
      "source": [
        "This code will help you to load google dive in the jupiter notebook and help to access the dataset stored in the drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8O0-fDoUdAFr"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ah24WwxoArBp"
      },
      "outputs": [],
      "source": [
        "DatasetLoc = '/content/gdrive/MyDrive/Dataset/'\n",
        "SelDataSet = 'IntelIC'\n",
        "NumberOfClass = 6\n",
        "data_dir = DatasetLoc+SelDataSet\n",
        "logFile = '/content/gdrive/MyDrive/Colab Notebooks/Logs'+'ConvModel.log'\n",
        "#savePATH\n",
        "SavePath = '/content/gdrive/MyDrive/Model/'+SelDataSet+'/VGG_Cifar_v2'\n",
        "LoadPath ='/content/gdrive/MyDrive/Model'+SelDataSet+'VGG_Cifar_v2'\n",
        "datafile = 'cifar10'\n",
        "DatasetPath = '/content/gdrive/MyDrive/Dataset/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWFVgnHPGoZC"
      },
      "source": [
        "Following cell will import basic numeric and ploting librrary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4moYb7-weO9"
      },
      "outputs": [],
      "source": [
        "#import basic library for some basic function\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from tqdm import tqdm\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFPcW4iNG1N3"
      },
      "source": [
        "Following Cell facilitate to read files from drive and and help to read dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7aISk4Kc23P"
      },
      "outputs": [],
      "source": [
        "# import library to perform file operation\n",
        "import os #use to access the files \n",
        "import tarfile # use to extract dataset from zip files\n",
        "import sys\n",
        "import zipfile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wrUgOg_HJCm"
      },
      "source": [
        "Torch Library provides facilities to create networl architechture and write farword and backwor phase od neural network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhl9_5SWwWuG"
      },
      "outputs": [],
      "source": [
        "#import torch library to build neural network\n",
        "import torch  # Elementory function of tensor is define in torch package\n",
        "import torch.nn as nn # Several layer architectur is define here\n",
        "import torch.nn.functional as F # loss function and activation function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JQILI1UHfA1"
      },
      "source": [
        "Computer vision is one of the most important application and thus lots od deplopment in the and torch.vision provides many facilities that can be use to imporve model such as data augmentation, reading data batchwise, suffling data before each epoch and many more\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lNwx_5HQwZnL"
      },
      "outputs": [],
      "source": [
        "# import torch library related to data processing\n",
        "import torchvision # provides facilities to access image dataset\n",
        "from torchvision.datasets.utils import download_url \n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import random_split\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision import datasets, models, transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeyFtxdf6h8o"
      },
      "source": [
        "#Data Prepration 1\n",
        "\n",
        "Based on the image size of the dataset choose apropriate values of the color channel and Image Size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gV-iqFhvIHeq"
      },
      "source": [
        "Here we can define path to a folder where we can keep all the dataset. In the following we are using the zip files. Originally dataset should be in the following format DataSetName is parent folder and it should contain train and test folder. train and test folder should contain folder for each category and images of respective category should be in the respective category folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2UyAyhEzcpn9",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "fullpath = '/content/gdrive/MyDrive/Dataset/IntelIC.zip'\n",
        "zip_ref = zipfile.ZipFile(fullpath, 'r') #Opens the zip file in read mode\n",
        "zip_ref.extractall('/tmp') #Extracts the files into the /tmp folder\n",
        "zip_ref.close()\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRAwsy-W7HCq"
      },
      "source": [
        "Choose an apropriate batch size that can be loaded in the current enviroment without crashing and also do not choose too big batch even if dataset is small because it leads to very few updates per epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UkTeSRNc5WF"
      },
      "outputs": [],
      "source": [
        "bs = 128\n",
        "ImageSize = 224"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbIUqcuyK5DK"
      },
      "source": [
        "Data Augmentaion generally help in reducing overfitting error during trainng process and thus we are performing randon horizontal flip and random crop during training but during validation as no training happens we dont perform data augmentation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9pEtuitrfrO"
      },
      "outputs": [],
      "source": [
        "# Data augmentation and normalization for training\n",
        "# Just normalization for validation\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(ImageSize),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "data_dir = '/tmp/'+SelDataSet\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
        "                                          data_transforms[x])\n",
        "                  for x in ['train', 'test']}\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=bs,\n",
        "                                             shuffle=True, num_workers=2)\n",
        "              for x in ['train', 'test']}\n",
        "\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'test']}\n",
        "class_names = image_datasets['train'].classes\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Bf_Ug-O2eie"
      },
      "source": [
        "Look inside the single batch of dataset images we are going to train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-hMYKSIsDNg"
      },
      "outputs": [],
      "source": [
        "def imshow(inp, title=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    inp = std * inp + mean\n",
        "    #inp = np.clip(inp, 0, 1)\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "\n",
        "\n",
        "# Get a batch of training data\n",
        "inputs, classes = next(iter(dataloaders['train']))\n",
        "\n",
        "# Make a grid from batch\n",
        "out = torchvision.utils.make_grid(inputs)\n",
        "\n",
        "imshow(out, title=[class_names[x] for x in classes])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDO9wMLhLs8_"
      },
      "source": [
        "Check if Cuda GPU is available"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3keAOLbeHBN"
      },
      "outputs": [],
      "source": [
        "#check for CUDA enabled GPU card\n",
        "def getDeviceType():\n",
        "  if torch.cuda.is_available():\n",
        "    return torch.device('cuda')\n",
        "  else:\n",
        "    return torch.device('cpu')\n",
        "\n",
        "\n",
        "device = getDeviceType()\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0iq-SdG8R1b"
      },
      "source": [
        "#Training Process\n",
        "Below code will work as a base function and provide all the important function like compute loss, accuracy and print result in a perticular formate afte each epoch. Funvtion are as follow\n",
        "1. Accuracy : Computer accuracy in evalutaion mode of pytorch on given dataset for given model\n",
        "2. compute_batch_loss : Compute batch loss and append the loss in the list of batch loss.\n",
        "3. compute_batch_loss_acc : Compute batch loss, batch accuracy and append the loss in the list of batch loss.\n",
        "4. accumulate_batch_loss_acc: Accumulate loss from the list of batch and acccuraly loss.\n",
        "5. Epoch end to print the output after every epoch in proper format\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_YnrEWpJgnu"
      },
      "outputs": [],
      "source": [
        "def accuracy(outputs, labels):\n",
        "# compute accuracy = total correct prediction / total number f ele\n",
        "\n",
        "    # torch.max(output, dim) will return max value and corresponding index, \n",
        "    # Here we are intersted only on index value as it indicate class\n",
        "    _, preds = torch.max(outputs, dim=1) # get the prediction vector\n",
        "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
        "\n",
        "# Compute loss of the given batch and return it\n",
        "def compute_batch_loss(newmodel, batch_X,batch_y):\n",
        "  images = batch_X.to(device)\n",
        "  labels = batch_y.to(device)\n",
        "  out = newmodel(images)                  # Generate predictions\n",
        "  loss = F.cross_entropy(out, labels) # Calculate loss\n",
        "  return loss\n",
        "\n",
        "# Computes loss and accuracy of the given batch(Used in validation)\n",
        "def compute_batch_loss_acc(newmodel, batch_X,batch_y):\n",
        "    images = batch_X.to(device)\n",
        "    labels = batch_y.to(device)\n",
        "    out = newmodel(images)                    # Generate predictionsin_features=4096\n",
        "    loss = F.cross_entropy(out, labels)   # Calculate loss\n",
        "    acc = accuracy(out, labels)           # Calculate accuracy\n",
        "    return {'val_loss': loss, 'val_acc': acc}\n",
        "\n",
        "# At the end of epoch accumulate all batch loss and batch accueacy    \n",
        "def accumulate_batch_loss_acc(outputs):\n",
        "    batch_losses = [x['val_loss'] for x in outputs]\n",
        "    epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
        "    batch_accs = [x['val_acc'] for x in outputs]\n",
        "    epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
        "    return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
        "\n",
        "def epoch_end(epoch, result):\n",
        "  # Print in given format \n",
        "  # Epoch [0], last_lr: 0.00278, train_loss: 1.2862, val_loss: 1.2110, val_acc: 0.6135\n",
        "  strResult = \"Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
        "      epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_acc'])\n",
        "  print(strResult)\n",
        "  #print(\"Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
        "  #   epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_acc']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFJ0Ar5wWF7m"
      },
      "source": [
        "#Define Archietecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QAaDqFuWKV6"
      },
      "outputs": [],
      "source": [
        "def conv_block(in_channels, out_channels, pool=False, kernalSize =3, padd = 1 ):\n",
        "    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=kernalSize, padding=padd), \n",
        "              nn.BatchNorm2d(out_channels), # Layerwise Noramlizarion\n",
        "              nn.ReLU(inplace=True)]\n",
        "    if pool: layers.append(nn.MaxPool2d(2)) \n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "def vgg_fc_layer(size_in, size_out):\n",
        "    layer = nn.Sequential(\n",
        "        nn.Dropout(p=0.6),\n",
        "        nn.Linear(size_in, size_out),\n",
        "        nn.BatchNorm1d(size_out),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "    return layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhD05j-EWTbZ"
      },
      "outputs": [],
      "source": [
        "class VGG_Net_16(nn.Module):\n",
        "    \n",
        "    def __init__(self, in_channels, num_classes):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        # VGG_BLOCK_1(03,64)    -> Input(3,64x64) -> Output(64,32x32)\n",
        "        self.conv11 = conv_block(in_channels, 64,pool=False)\n",
        "        self.conv12 = conv_block(64, 64,pool=True) #112x112\n",
        "\n",
        "        # VGG_BLOCK_2(64,128)   -> Input(64,32x32) -> Output(128,16x16)\n",
        "        self.conv21 = conv_block(64,128,pool=False)\n",
        "        self.conv22 = conv_block(128, 128, pool=True) #56x56                             \n",
        "        \n",
        "        # VGG_BLOCK_3(128,128) -> Input(128,16x16) -> Output(128,16x16)\n",
        "        self.conv31 = conv_block(128,256,pool=False)\n",
        "        self.conv32 = conv_block(256, 256, pool=False)  #28x28\n",
        "        self.conv33 = conv_block(256, 256, pool=True)\n",
        "        \n",
        "        # VGG_BLOCK_4(256,512)  -> Input(256,8x8) -> Output(512x4x4)\n",
        "        self.conv41 = conv_block(256, 512, pool=False)                              \n",
        "        self.conv42 = conv_block(512, 512, pool=False)\n",
        "        self.conv43 = conv_block(512, 512, pool=True)  #14x14\n",
        "\n",
        "        # VGG_BLOCK_5(512,512) -> Input(512x4x4) -> Output(512x4x4)\n",
        "        self.conv51 = conv_block(512, 512, pool=False)                              \n",
        "        self.conv52 = conv_block(512, 512, pool=False)\n",
        "        self.conv53 = conv_block(512, 512, pool=True)  #7x7\n",
        "\n",
        "          \n",
        "        # Classifier Part Of VGG_Net\n",
        "        self.FC1 = vgg_fc_layer(7*7*512, 4096)\n",
        "        self.FC2 = vgg_fc_layer(4096, 4096)\n",
        "        self.output = vgg_fc_layer(7*7*512, 4096)\n",
        "    \n",
        "    def forward(self, xb):\n",
        "        out = self.conv11(xb)\n",
        "        out = self.conv12(out)\n",
        "        \n",
        "        out = self.conv21(out)\n",
        "        out = self.conv22(out)\n",
        "\n",
        "        out = self.conv31(out)\n",
        "        out = self.conv32(out)\n",
        "        out = self.conv33(out)\n",
        "\n",
        "        out = self.conv41(out)\n",
        "        out = self.conv42(out)\n",
        "        out = self.conv43(out)\n",
        "\n",
        "        out = self.conv51(out)\n",
        "        out = self.conv52(out)\n",
        "        features = self.conv53(out)\n",
        "\n",
        "        out = features.view(out.size(0), -1)  # Flatten the layer\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvkFO3m9WVTn",
        "outputId": "1146f43c-aed4-4444-d2a4-192d1d2cad9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VGG_Net_16(\n",
            "  (conv11): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "  )\n",
            "  (conv12): Sequential(\n",
            "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv21): Sequential(\n",
            "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "  )\n",
            "  (conv22): Sequential(\n",
            "    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv31): Sequential(\n",
            "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "  )\n",
            "  (conv32): Sequential(\n",
            "    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "  )\n",
            "  (conv33): Sequential(\n",
            "    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv41): Sequential(\n",
            "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "  )\n",
            "  (conv42): Sequential(\n",
            "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "  )\n",
            "  (conv43): Sequential(\n",
            "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv51): Sequential(\n",
            "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "  )\n",
            "  (conv52): Sequential(\n",
            "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "  )\n",
            "  (conv53): Sequential(\n",
            "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (FC1): Sequential(\n",
            "    (0): Dropout(p=0.6, inplace=False)\n",
            "    (1): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "    (2): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (3): ReLU()\n",
            "  )\n",
            "  (FC2): Sequential(\n",
            "    (0): Dropout(p=0.6, inplace=False)\n",
            "    (1): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (2): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (3): ReLU()\n",
            "  )\n",
            "  (output): Sequential(\n",
            "    (0): Dropout(p=0.6, inplace=False)\n",
            "    (1): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "    (2): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (3): ReLU()\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "#NumberOfClass = 6\n",
        "MyNewModel = VGG_Net_16(3,NumberOfClass)\n",
        "print(MyNewModel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cm5VSgMfwYDw",
        "outputId": "669566d1-3adc-44aa-c432-6fe1ecd5c59b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (13): ReLU(inplace=True)\n",
            "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (18): ReLU(inplace=True)\n",
            "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (20): ReLU(inplace=True)\n",
            "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (25): ReLU(inplace=True)\n",
            "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (27): ReLU(inplace=True)\n",
            "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "newModel = torchvision.models.vgg16(pretrained=True)\n",
        "print(newModel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTpU0xqLwYDx"
      },
      "outputs": [],
      "source": [
        "#[0,2, 5,7, 10,12,14, 17,19,21, 24,26,28]\n",
        "MyNewModel.conv11[0].weight = newModel.features[0]._parameters['weight']\n",
        "MyNewModel.conv12[0].weight = newModel.features[2]._parameters['weight']\n",
        "\n",
        "MyNewModel.conv21[0].weight = newModel.features[5]._parameters['weight']\n",
        "MyNewModel.conv22[0].weight = newModel.features[7]._parameters['weight']\n",
        "\n",
        "MyNewModel.conv31[0].weight = newModel.features[10]._parameters['weight']\n",
        "MyNewModel.conv32[0].weight = newModel.features[12]._parameters['weight']\n",
        "MyNewModel.conv33[0].weight = newModel.features[14]._parameters['weight']\n",
        "\n",
        "MyNewModel.conv41[0].weight = newModel.features[17]._parameters['weight']\n",
        "MyNewModel.conv42[0].weight = newModel.features[19]._parameters['weight']\n",
        "MyNewModel.conv43[0].weight = newModel.features[21]._parameters['weight']\n",
        "\n",
        "MyNewModel.conv51[0].weight = newModel.features[24]._parameters['weight']\n",
        "MyNewModel.conv52[0].weight = newModel.features[26]._parameters['weight']\n",
        "MyNewModel.conv53[0].weight = newModel.features[28]._parameters['weight']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W53797PgUrQM"
      },
      "source": [
        "## Define Training \n",
        "Here we will evalute our model after each epoch on validation dataset using evalute method\n",
        "get_lr method returnd last learning rate used in the training\n",
        "Here we are using one fit cycle method in which we specify the max learning rate and learning rate start from 1/10th value of max_lr and slowly increases the value to max_lr for 40% of updates then decreases to its initial value for 40% updates and then further decreases to 1/100th of max_lr value to perform final fine tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SskViaSQjHUR"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "# evalute model on given dataset using given data loader\n",
        "def evaluate(model, data_loader):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      for batch_X, batch_y in data_loader:\n",
        "        outputs = [compute_batch_loss_acc(model,batch_X,batch_y)]\n",
        "      return accumulate_batch_loss_acc(outputs)\n",
        "\n",
        "# Use special scheduler to change the value of learning rate\n",
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']\n",
        "\n",
        "# epoch=8, max_lr=.01, weight_decay(L2-Regu parametr)=.0001,opt_func=Adam\n",
        "\n",
        "\n",
        "def fit_one_cycle(ModelName,epochs, max_lr, model, \n",
        "                  weight_decay=0, L1=0,grad_clip=None, opt_func=torch.optim.SGD):\n",
        "    torch.cuda.empty_cache()\n",
        "    history = []\n",
        "    \n",
        "    # Set up cutom optimizer here we will use one cycle scheduler with max learning\n",
        "    # rate given by max_lr, default optimizer is SGD but we will use ADAM, and \n",
        "    # L2 Regularization using weight decay\n",
        "    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n",
        "    \n",
        "    # Set up one-cycle learning rate scheduler\n",
        "    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, \n",
        "                                                steps_per_epoch=len(dataloaders['train']))\n",
        "    \n",
        "    print(\"Training Starts\")\n",
        "    with open(\"/content/gdrive/My Drive/data/ConvModel.log\",\"a\") as f:\n",
        "      for epoch in range(epochs):\n",
        "          # Training Phase \n",
        "          model.train()  #######################\n",
        "          train_losses = []\n",
        "          lrs = []\n",
        "          #for batch in train_loader:\n",
        "          for batch_X, batch_y in dataloaders['train']:\n",
        "              \n",
        "              # computer the training loss of current batch\n",
        "              loss = compute_batch_loss(model,batch_X,batch_y)\n",
        "              \n",
        "              l1_crit = nn.L1Loss()\n",
        "              reg_loss = 0\n",
        "              for param in model.parameters():\n",
        "                reg_loss += l1_crit(param,target=torch.zeros_like(param))\n",
        "\n",
        "              loss += L1*reg_loss \n",
        "              train_losses.append(loss)\n",
        "              loss.backward() # compute the gradient of all weights\n",
        "              \n",
        "              # Clip the gradient value to maximum allowed grad_clip value\n",
        "              if grad_clip: \n",
        "                  nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
        "              \n",
        "              optimizer.step() # Updates weights \n",
        "              # pytorch by default accumulate grade history and if we dont want it\n",
        "              # we should make all previous grade value equals to zero\n",
        "              optimizer.zero_grad() \n",
        "              \n",
        "              # Record & update learning rate\n",
        "              lrs.append(get_lr(optimizer))\n",
        "              sched.step() # Update the learning rate\n",
        "          \n",
        "          # Compute Validation Loss and Valodation Accuracy\n",
        "          result = evaluate(model, dataloaders['test'])\n",
        "\n",
        "          # Compute Train Loss of whole epoch i.e mean of loss of batch \n",
        "          result['train_loss'] = torch.stack(train_losses).mean().item()\n",
        "\n",
        "          # Observe how learning rate is change by schedular\n",
        "          result['lrs'] = lrs\n",
        "\n",
        "          # print the observation of each epoch in a proper format\n",
        "          epoch_end(epoch, result) \n",
        "          strResult = \"Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_acc'])\n",
        "          f.write(f\"{ModelName}-\\t{strResult}\\n\")\n",
        "          history.append(result) # append tupple result with val_acc, vall_loss, and trin_loss\n",
        "        \n",
        "    return history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxRbOmgH-EBY"
      },
      "source": [
        "Define the function that help is evaluating the model such as \n",
        "1. acuracy vs eopch to see that whether performance is improving or not during training, \n",
        "2. model loss vs epoch to see the loos value improvment during each epoch and \n",
        "3. learning rate plot to see how learning rate changes during each updates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThCTWdQfusP0"
      },
      "outputs": [],
      "source": [
        "# Plot Validation accuracy Over Epoch\n",
        "def plot_accuracies(history):\n",
        "    accuracies = [x['val_acc'] for x in history]\n",
        "    plt.plot(accuracies, '-x')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.title('Accuracy vs. No. of epochs');\n",
        "\n",
        "# Plot training and validation loss to check for overfitting and underfitting\n",
        "def plot_losses(history):\n",
        "    train_losses = [x.get('train_loss') for x in history]\n",
        "    val_losses = [x['val_loss'] for x in history]\n",
        "    plt.plot(train_losses, '-bx')\n",
        "    plt.plot(val_losses, '-rx')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('loss')\n",
        "    plt.legend(['Training', 'Validation'])\n",
        "    plt.title('Loss vs. No. of epochs');\n",
        "\n",
        "# plot the learning rate values over each batch\n",
        "def plot_lrs(history):\n",
        "    lrs = np.concatenate([x.get('lrs', []) for x in history])\n",
        "    plt.plot(lrs)\n",
        "    plt.xlabel('Batch no.')\n",
        "    plt.ylabel('Learning rate')\n",
        "    plt.title('Learning Rate vs. Batch no.');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bi_p_6OfQMRg"
      },
      "source": [
        "we can download pretrain model and perform fine tuning on the model with fix feature extracter using the following"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02kAYnmJl-56"
      },
      "outputs": [],
      "source": [
        "#################### Download Pretrain VGGNet ################\n",
        "'''\n",
        "newModel = torchvision.models.vgg16(pretrained=True)\n",
        "for param in newModel.parameters():\n",
        "    param.requires_grad = False\n",
        "#print(newModel)\n",
        "#Need to change the below code if we choose different model\n",
        "print(newModel.classifier[6])\n",
        "num_ftrs = newModel.classifier[6].in_features\n",
        "\n",
        "# Here the size of each output sample is set to 10.\n",
        "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
        "newModel.classifier[6] = nn.Linear(num_ftrs, NumberOfClass)\n",
        "newModel = newModel.to(device)\n",
        "'''\n",
        "##################### Download Pretrain ResNet 18 ############################\n",
        "\n",
        "'''\n",
        "newModel = torchvision.models.resnet18(pretrained=True)\n",
        "print(newModel.fc)\n",
        "for param in newModel.parameters():\n",
        "    param.requires_grad = False\n",
        "#print(newModel)\n",
        "#Need to change the below code if we choose different model\n",
        "num_ftrs = newModel.fc.in_features\n",
        "# Here the size of each output sample is set to 10.\n",
        "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
        "newModel.fc = nn.Linear(num_ftrs, 10)\n",
        "newModel = newModel.to(device)\n",
        "'''\n",
        "################# Load Stored Trained Model #####################################\n",
        "\n",
        "PATH ='/content/gdrive/MyDrive/Model/IntelIC/VGG_Cifar_v2'\n",
        "if device== torch.device('cpu'):\n",
        "    newModel = torch.load(PATH, map_location=torch.device('cpu'))\n",
        "else:\n",
        "    newModel = torch.load(PATH, map_location=torch.device('cuda'))\n",
        "\n",
        "# Need to change the following loop for dofferent model\n",
        "count=0\n",
        "for param in newModel.parameters():\n",
        "    print(count,\" : \",param.shape)\n",
        "    if count in (16,18,20):\n",
        "      param.requires_grad=True\n",
        "      #print(param.requires_grad)\n",
        "    else:\n",
        "      param.requires_grad=False\n",
        "      #print(param.requires_grad)\n",
        "    count +=1\n",
        "\n",
        "print(newModel)\n",
        "#################################################################################\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xlNtqnONjuXx"
      },
      "outputs": [],
      "source": [
        "count = 0\n",
        "for param_tensor in newModel.state_dict():\n",
        "    print(count,\":\\t\",param_tensor, \"\\t\", newModel.state_dict()[param_tensor].size())\n",
        "    count+=1\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOCH1xjzQKXI"
      },
      "source": [
        "During the next itteration if we had a better model stored in drive we can load model and continue training by learning parameter of all dense layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUCW1BYeWfAP"
      },
      "source": [
        "Initialise all the hyper meter required for training such as learning rate that control the step size, gradient clipping value that limits the maximum value of grads, L2 regularization using weight decay value, L1 regularization using L1 value and then choose apropriate optimization funtion for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kcc8mZYGXuMg"
      },
      "outputs": [],
      "source": [
        "historyfull = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40ofhmStDD3b"
      },
      "source": [
        "###In the below cell we are setting following hyper-parameter\n",
        "1. Epochs = Number of epoch we want to perform on dataset\n",
        "2. Max_Lr = indicate maximum learning rate\n",
        "3. grad clip = to control the maximum value of gradient\n",
        "4. weight decay = to do L2 regularization on model\n",
        "5. L1 regularization constant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-PENJzVkeio"
      },
      "outputs": [],
      "source": [
        "historylast = []\n",
        "epochs = 15\n",
        "# We are using one cycle fit function in which learning rate start with 1/10th \n",
        "# of selected maximum learning rate and increase learning rate from min to max\n",
        "# in 1st phase and then decrease from max to min in 2nd phase\n",
        "max_lr = 3e-4\n",
        "grad_clip = 0.3 # Maximum allowed value of grad: grad = min(grad,0.01)\n",
        "# weight decay is a constant multiplier for L2 regularization\n",
        "weight_decay = 1e-6\n",
        "L1 = 1e-6\n",
        "# Choosing Adma optimizer as try to implement SGD+Momentum+Adaptive Learining\n",
        "opt_func = torch.optim.Adam\n",
        "\n",
        "MODEL_NAME = f\"VGG_Net-MLR-\\t{max_lr}-GC{grad_clip}-WD-{weight_decay}-L1-{L1}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fo00eoyBKZS"
      },
      "source": [
        "Start the training using one fit cycle algorithms where learning rate start with one tenth of provided max_lr and then increase it value till that point and then decrease onword and for last few epoch learning rate furthe decrease"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "265hesKSXbnA"
      },
      "source": [
        "Pass the hyperparameter for training of the model and start training process for set number of epcoh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwQMNy8Am8ON"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "historylast += fit_one_cycle(MODEL_NAME,epochs, max_lr, newModel,  \n",
        "                             grad_clip=grad_clip, \n",
        "                             weight_decay=weight_decay, L1=L1,\n",
        "                             opt_func=opt_func\n",
        "                             )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fQY0wX6coby"
      },
      "outputs": [],
      "source": [
        "for item in historylast:\n",
        "  historyfull.append(item)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9T4EunVJY-Fm"
      },
      "source": [
        "Evalute model for accuracy for last training run "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WuB1YFxJu1LC"
      },
      "outputs": [],
      "source": [
        "print(\"Accuracy in last run\")\n",
        "plot_accuracies(historylast)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZO4CE6VZCF3"
      },
      "source": [
        "Evalute Models for whole training process for accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAlXPExrdJoX"
      },
      "outputs": [],
      "source": [
        "print(\"Accuracy is all run\")\n",
        "plot_accuracies(historyfull)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2VyfpFTZl-t"
      },
      "source": [
        "Evalute last run for loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aOWKnBplu6N1"
      },
      "outputs": [],
      "source": [
        "print(\"Loss of last run\")\n",
        "plot_losses(historylast)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqa_JvMEZqQ4"
      },
      "source": [
        "Evalute full training process for loss value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7iRkJWldZYz"
      },
      "outputs": [],
      "source": [
        "print(\"Loss in All run\")\n",
        "plot_losses(historyfull)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7B1s2QRZyfu"
      },
      "source": [
        "See how the learning rate changedin process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHRvjqpIdu1g"
      },
      "outputs": [],
      "source": [
        "print(\"Learning rate of all run\")\n",
        "plot_lrs(historyfull)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjO1EIRmaFTN"
      },
      "source": [
        "Print validation accuracy for all epoch of last run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHX-59zmvAa1"
      },
      "outputs": [],
      "source": [
        "accuracies = [x['val_acc']*100 for x in historylast]\n",
        "accuracies = [\"%.2f\" % v for v in accuracies]\n",
        "print(accuracies)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WB7netlsaQ-z"
      },
      "source": [
        "##Define method to compute accuracy for a given model on given dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAbjUs6s09dq"
      },
      "outputs": [],
      "source": [
        "print(SavePath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmLa4W_XvEc_"
      },
      "outputs": [],
      "source": [
        "def accuraciesTotal(newModel, data_loader):\n",
        "  with torch.no_grad():\n",
        "    acc = []\n",
        "    for batch in data_loader:\n",
        "        images, label = batch\n",
        "        images, labels = batch[0].to(device), batch[1].to(device)\n",
        "        out = newModel(images)\n",
        "        acc.append(accuracy(out, labels))\n",
        "        \n",
        "    return torch.mean(torch.stack(acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GM_10FNcakA"
      },
      "source": [
        "###Prepare the data loader for inference. During the inference we want to perform same tranformation on test and train dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ilAaxUd4aYH"
      },
      "outputs": [],
      "source": [
        "image_datasets_eval = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
        "                                          data_transforms['test'])\n",
        "                  for x in ['train', 'test']}\n",
        "\n",
        "dataloaders_eval = {x: torch.utils.data.DataLoader(image_datasets_eval[x], batch_size=bs,\n",
        "                                             shuffle=False, num_workers=2)\n",
        "                  for x in ['train', 'test']}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzfcV_XFa2aK"
      },
      "source": [
        "###Evalute final accuracy of test and train dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-C3bwmUvGpF"
      },
      "outputs": [],
      "source": [
        "trainacc = accuraciesTotal(newModel,dataloaders_eval['train'])\n",
        "testacc = accuraciesTotal(newModel,dataloaders_eval['test'])\n",
        "print(\"Train Acc :\" + str(trainacc) + \"\\nTest  Acc :\" +str(testacc))\n",
        "#print(testacc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOA9OrCma-xr"
      },
      "source": [
        "Save the model if we feel we have made some improvement in the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BL8Gi0TVtvHo"
      },
      "outputs": [],
      "source": [
        "#PATH = \"/content/gdrive/My Drive/Model/cifar10/VGG_Cifar_P3\"\n",
        "torch.save(newModel, SavePath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFtnGMlM0NBy"
      },
      "outputs": [],
      "source": [
        "print(logFile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INlOdiUrzJ6L"
      },
      "outputs": [],
      "source": [
        "#outfile = LogLoc+\"FinalOut.log\"\n",
        "\n",
        "with open(logFile,\"a\") as f2:\n",
        "  f2.write(\"Train Acc :\" + str(trainacc) + \"\\nTest  Acc :\" +str(testacc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xq3uqoZm87Bb"
      },
      "source": [
        "# Below code shows basic ln structure pruning methods provided by the pytorch. Here if we want to apply custom pruning then we need to write our own compute_mask function "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vy969sMrwhq1"
      },
      "outputs": [],
      "source": [
        "#import library to facilitate pruning\n",
        "import torch.nn.utils.prune as prune\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmTc1wc1fdPf"
      },
      "source": [
        "###Select the features want to prune and store them in the list of of Mudule here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvbUgmAzabDG"
      },
      "outputs": [],
      "source": [
        "print(newModel.features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8bN9ydRM0rn"
      },
      "outputs": [],
      "source": [
        "Module = []\n",
        "for i in (0,3,6,8,11,13,16,18):\n",
        "  Module.append(newModel.features[i])\n",
        "\n",
        "for i in range(len(Module)):\n",
        "  print(Module[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbASVDIqf2Z_"
      },
      "source": [
        "Select the amount of feature we want to prune in each Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9-mPS4MLnF5"
      },
      "outputs": [],
      "source": [
        "pr = [.05,.05,.1,.1,.15,.15,.15,.20]\n",
        "for i in range(len(Module)):\n",
        "  prune.ln_structured(Module[i], name=\"weight\", amount=pr[i], n=1, dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNe43hAOjuX-"
      },
      "outputs": [],
      "source": [
        "numberOfZero = 0\n",
        "numberOfElements = 0\n",
        "for i in range(len(Module)):\n",
        "    numberOfZero += torch.sum(Module[i].weight == 0)\n",
        "    numberOfElements += Module[i].weight.nelement()\n",
        "    print(\n",
        "        \"Sparsity in conv1.weight: {:.2f}%\".format(\n",
        "            100. * float(torch.sum(Module[i].weight == 0))\n",
        "            / float(Module[i].weight.nelement())\n",
        "        )\n",
        "    )    \n",
        "       \n",
        "print(\"Global Sparsity: {:.2f}%\".format(100*float(numberOfZero)/float(numberOfElements)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5v2py6AWYAGe"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "prune.remove(model.conv11[0], 'weight')\n",
        "prune.remove(model.conv12[0], 'weight')\n",
        "\n",
        "prune.remove(model.conv22[0], 'weight')\n",
        "prune.remove(model.conv21[0], 'weight')\n",
        "\n",
        "prune.remove(model.conv31[0], 'weight')\n",
        "prune.remove(model.conv32[0], 'weight')\n",
        "prune.remove(model.conv33[0], 'weight')\n",
        "\n",
        "prune.remove(model.conv41[0], 'weight')\n",
        "prune.remove(model.conv42[0], 'weight')\n",
        "prune.remove(model.conv43[0], 'weight')\n",
        "\n",
        "prune.remove(model.conv51[0], 'weight')\n",
        "prune.remove(model.conv52[0], 'weight')\n",
        "prune.remove(model.conv53[0], 'weight')\n",
        "'''\n",
        "for i in (0,3,6,8,11,13,16,18):\n",
        "  prune.remove(newModel.features[i], 'weight')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FkGCGhmsbF1V"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "06 VGG16TranferLearningPruning.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}