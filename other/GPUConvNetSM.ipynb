{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oI1lbXbFrDXv"
   },
   "source": [
    "#### Select the propeties of image data such as (ImageSize X ImageSize)  and Color Channel (3=RGB or 1=Grayscale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 7125,
     "status": "ok",
     "timestamp": 1608617263447,
     "user": {
      "displayName": "Pragnesh Thaker",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgLVRXB_uzLSPlOFa1Byq4JroZuZLCvAi3nnmje=s64",
      "userId": "16372140651231488017"
     },
     "user_tz": -330
    },
    "id": "cgc3f62jP1jZ"
   },
   "outputs": [],
   "source": [
    "# import library to perform file operation\n",
    "import os #use to access the files \n",
    "import tarfile # use to extract dataset from zip files\n",
    "import sys\n",
    "\n",
    "#import torch library to build neural network\n",
    "import torch  # Elementory function of tensor is define in torch package\n",
    "import torch.nn as nn # Several layer architectur is define here\n",
    "import torch.nn.functional as F # loss function and activation function\n",
    "\n",
    "# import torch library related to data processing\n",
    "import torchvision # provides facilities to access image dataset\n",
    "from torchvision.datasets.utils import download_url \n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import random_split\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from tqdm import tqdm\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 7124,
     "status": "ok",
     "timestamp": 1608617263450,
     "user": {
      "displayName": "Pragnesh Thaker",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgLVRXB_uzLSPlOFa1Byq4JroZuZLCvAi3nnmje=s64",
      "userId": "16372140651231488017"
     },
     "user_tz": -330
    },
    "id": "eigqZQGMh5fm"
   },
   "outputs": [],
   "source": [
    "colorChannel = 3\n",
    "ImageSize    = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24gu7lTi5OgK"
   },
   "source": [
    "#### Import Library to perform File operation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FwDFvgUgz2Gi"
   },
   "source": [
    "### Mount your google with secure authentication\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FBGkLFHhh7-e"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kuy8SAkokHZ5"
   },
   "source": [
    "### Load dataset from the google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YA5o4tJGh-HT"
   },
   "outputs": [],
   "source": [
    "PATH = '/content/gdrive/My Drive/Colab Notebooks/Dataser/IntelIC'\n",
    "sys.path.append('/content/gdrive/My Drive/Colab Notebooks/Dataser/IntelIC')\n",
    "training_data = np.load(\"/content/gdrive/My Drive/Colab Notebooks/Dataset/IntelIC/training_data3-64.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eGAEWXHh0Jcl"
   },
   "source": [
    "#### Normalise the input between 0 to 1 as normalize input tend to converge faster and gives better result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "adD2mVUIqhRq"
   },
   "outputs": [],
   "source": [
    "X = torch.Tensor([i[0] for i in training_data]).view(-1,colorChannel,ImageSize,ImageSize)\n",
    "X = X/255.0\n",
    "y = ([i[1] for i in training_data])\n",
    "#y = np.argmax(y, axis=1) # Convert one hot representation to class index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "95Z43Mas5gzV"
   },
   "source": [
    "#### Create Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C1v6suJwqW8G"
   },
   "outputs": [],
   "source": [
    "VAL_PCT = 0.1  # lets reserve 10% of our data for validation\n",
    "val_size = int(len(X)*VAL_PCT)\n",
    "print(val_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uhC-hUrp5p7s"
   },
   "source": [
    "#### Spliting Data into validation data and training data and creating minibatches of selected sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mgrhuT-Lqpjz"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "train_X = X[:-val_size]\n",
    "train_y = y[:-val_size]\n",
    "train_X  = train_X.clone().detach()\n",
    "train_y  = torch.tensor(train_y)\n",
    "\n",
    "test_X = X[-val_size:]\n",
    "test_y = y[-val_size:]\n",
    "test_X  = test_X.clone().detach()\n",
    "test_y  = torch.tensor(test_y)\n",
    "\n",
    "train_ds = TensorDataset(train_X, train_y)\n",
    "test_ds = TensorDataset(test_X, test_y)\n",
    "\n",
    "batch_size =400\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "test_dl = DataLoader(test_ds, 2*batch_size, shuffle=False)\n",
    "\n",
    "print(\"Number of element in Train Dataset:\",len(train_X)) \n",
    "print(\"Numberof element in Test Dataset\",len(test_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7sMEGZfY0qiK"
   },
   "source": [
    "#### Now as our data is fully ready now build and train the model.\n",
    "\n",
    "#### 1st import nn to define architecture\n",
    "\n",
    "#### import nn.function to select activation function and loss function\n",
    "\n",
    "#### Check if GPU is available and if yes run training on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P-SfPlZCh5gZ"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bhaFIudO0zuk"
   },
   "source": [
    "#### Build the architecture of the Neural Network by selecting different kinds of layer\n",
    "\n",
    "#### I am using all convolution layer of 3x3 filter with pading 1 i.e same conv\n",
    "\n",
    "##### C1->C2->C3->P1->C4->C5->P2->C6->C7-C8(1X1)->FC1-OutLayer\n",
    "\n",
    "#### C8 is 1x1 convolution layer that is used to reduce number of parameter\n",
    "\n",
    "#### Defined evalOnTrain method ro test the model on Training dataset\n",
    "\n",
    "#### Defined evalOnVal method ro test the model on Validation dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8YImLkSqh5gl"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "        self.colorChannel = 3\n",
    "\n",
    "        # 1st block to cover 7x7 preceptive field\n",
    "        self.conv1 = nn.Conv2d(colorChannel, 32, kernel_size=3,stride=1,padding=1) \n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3,stride=1,padding=1) \n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=3,stride=1,padding=1)\n",
    "\n",
    "        # 2nd block to cover 5x5 preceptive field\n",
    "        self.conv4 = nn.Conv2d(32, 64, kernel_size=3,stride=1,padding=1)\n",
    "        self.conv5 = nn.Conv2d(64, 64, kernel_size=3,stride=1,padding=1)\n",
    "\n",
    "        self.conv6 = nn.Conv2d(64, 128, kernel_size=3,stride=1,padding=1)\n",
    "        self.conv7 = nn.Conv2d(128, 256, kernel_size=3,stride=1,padding=1)\n",
    "\n",
    "        self.conv8 = nn.Conv2d(256, 512, kernel_size=3,stride=1,padding=1)\n",
    "\n",
    "        self.dropout1 = nn.Dropout2d(0)\n",
    "        self.dropout2 = nn.Dropout2d(0)\n",
    "        x = torch.randn(colorChannel,ImageSize,ImageSize).view(-1,colorChannel,ImageSize,ImageSize)\n",
    "        self._to_linear = None\n",
    "        self.convs(x)\n",
    "        \n",
    "        self.fc1 = nn.Linear(self._to_linear, 256) #flattening.\n",
    "        self.fc2 = nn.Linear(256, 6) \n",
    "\n",
    "    def convs(self, x):\n",
    "        # 1st block\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x0 = x\n",
    "        x = F.relu(self.conv2(x0)) \n",
    "        x = F.relu(self.conv3(x)+x0)\n",
    "        x = F.max_pool2d(x,kernel_size=3,stride=2)\n",
    "\n",
    "        # 2nd block\n",
    "        x0 = x\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.relu(self.conv5(x)+x0)\n",
    "        x = F.max_pool2d( x,kernel_size=3,stride=2)\n",
    "        \n",
    "        \n",
    "        x = F.relu(self.conv6(x))\n",
    "        x = F.max_pool2d(x,kernel_size=3,stride=2)\n",
    "\n",
    "        x = F.relu( (self.conv7(x)))\n",
    "        x = F.max_pool2d( x,kernel_size=3,stride=2)\n",
    "        \n",
    "        x = F.relu(self.conv8(x))\n",
    "        x = F.max_pool2d( x,kernel_size=3,stride=2)\n",
    "     \n",
    "        x = self.dropout1(x)\n",
    "        if self._to_linear is None:# below code must run only once\n",
    "            self._to_linear = x[0].shape[0]*x[0].shape[1]*x[0].shape[2]\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convs(x)\n",
    "\n",
    "        # view is reshape function of pytorch used as flatten function\n",
    "        x = x.view(-1, self._to_linear)  \n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        # Special activation is required for output layer but as we are using\n",
    "        # crossEntropy loss it internally calls softmax and thus no activation req\n",
    "        x = self.fc2(x) \n",
    "        return x\n",
    "        \n",
    "    def evalute(self, Xset, yset):\n",
    "      Xset.to(device)\n",
    "      yset.to(device)\n",
    "      correct = 0\n",
    "      total = 0\n",
    "      with torch.no_grad():\n",
    "          for i in (range(len(Xset))):\n",
    "              real_class =yset[i].to(device)\n",
    "              net_out = net(Xset[i].view(-1, colorChannel, ImageSize, ImageSize).to(device))[0]  # returns a list, \n",
    "              predicted_class = torch.argmax(net_out)\n",
    "\n",
    "              if predicted_class == real_class:\n",
    "                  correct += 1\n",
    "              total += 1\n",
    "      return round(correct/total, 3)*100\n",
    "    \n",
    "    def computeBatchLoss(self,Xset,yset):\n",
    "      with torch.no_grad():\n",
    "        if torch.cuda.is_available():\n",
    "          Xset.to(device)\n",
    "          yset.to(device)\n",
    "          Xset, yset = Xset.cuda(), yset.cuda() \n",
    "        #net.zero_grad()\n",
    "        outputs = net(Xset)\n",
    "        loss = 0\n",
    "        loss = F.cross_entropy(outputs,yset)\n",
    "        \n",
    "      return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N7mNYroKkCmU"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"Running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rUZjXFxb8Uwh"
   },
   "source": [
    "Move the model to device memory (i.e GPU Memory if GPU is available else does nothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iK1xvt61h5gx"
   },
   "outputs": [],
   "source": [
    "net = Net()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6M5NChNDhWg"
   },
   "source": [
    "#### Choose appropraite optimization function and choose proper learning rate.\n",
    "\n",
    "#### Most of the time ADAM oprimizer is best choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dj0R5ndkh5g7"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "#import math\n",
    "max_lr = 5e-3\n",
    "optimizer = optim.Adam(net.parameters(), lr=max_lr, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cSDAOt-lhpYV"
   },
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y8d-_ONBfxyn"
   },
   "outputs": [],
   "source": [
    "trainHistory = []\n",
    "trainLoss = []\n",
    "testHistory = []\n",
    "testLoss = []\n",
    "lrHistory = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UXGSUFc-kThq"
   },
   "outputs": [],
   "source": [
    "startEPOCHS = 0\n",
    "count = 10\n",
    "grad_clip = 0.2\n",
    "stepCount = 0\n",
    "print(\"Model training starts\")\n",
    "sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=count,\n",
    "                                            steps_per_epoch=(len(train_dl) )  )\n",
    "\n",
    "for epoch in (range(startEPOCHS,(startEPOCHS + count))):\n",
    "    batchLoss = []\n",
    "    # We access the training data as a list of several minibatch stored in train_dl list\n",
    "    for batch_X, batch_y in train_dl:\n",
    "    #for batch_X,batch_y in enumerate(train_dl, 0):   \n",
    "        # Move the batch of data to the device memory i.e GPU memory is present\n",
    "        batch_X = batch_X.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "\n",
    "        # Pytorch accumulate the history of gradient and thus when we dont want to\n",
    "        # accumulate previous history we must make it zero \n",
    "        net.zero_grad()\n",
    "        outputs = net(batch_X)\n",
    "        \n",
    "        # Choose an apropraite loss function here we going to use cross entropy\n",
    "        loss = F.cross_entropy(outputs,batch_y)\n",
    "        batchLoss.append(loss)\n",
    "\n",
    "        #loss = F.mse_loss(batch_y,outputs) \n",
    "        \n",
    "        # below method performs back-propogation to compute gradient\n",
    "        loss.backward()  \n",
    "        \n",
    "        # For few parameter value of gradient may be very high and overshoot the\n",
    "        # optimal solution and so to avoid it we can define maximum allowed value \n",
    "        # for gradient and if gradient value is above it then it will bw clipped to max value\n",
    "        if grad_clip: \n",
    "                nn.utils.clip_grad_value_(net.parameters(), grad_clip)\n",
    "\n",
    "        optimizer.step()  # Does the update using selected optimizer\n",
    "\n",
    "        lrHistory.append(get_lr(optimizer))\n",
    "        sched.step()\n",
    "    \n",
    "    # uncomment below line if you want to save model in drive\n",
    "    '''\n",
    "    path = \"/content/gdrive/My Drive/Colab Notebooks/TrainedModel/IntelIC\" + str(epoch)\n",
    "    torch.save(net,path)\n",
    "    '''\n",
    "    # Maintain the history of accuracy to see the growt of given model\n",
    "    #trainHistory.append(net.evalute(train_X,train_y))\n",
    "    epochLoss = sum(batchLoss) / len(batchLoss)\n",
    "    trainLoss.append(epochLoss)\n",
    "\n",
    "\n",
    "    testHistory.append(net.evalute(test_X,test_y))\n",
    "    testLossList = []\n",
    "    for batch_X, batch_y in test_dl:\n",
    "      testLossList.append(net.computeBatchLoss(batch_X, batch_y))\n",
    "    \n",
    "    testLoss.append(sum(testLossList)/len(testLossList))\n",
    "    #print(f\"Training Accuracy : {trainHistory[-1]} Training Loss : {trainLoss[-1]}\")\n",
    "    #print(f\"Testing Accuracy : {testHistory[-1]}  Testing loss : {testLoss[-1]}\")\n",
    "    print(f\"Epoch: {epoch}. Training Loss: {epochLoss}\")\n",
    "    print(f\"Epoch: {epoch}. Testing Loss: {testLoss[-1]}\")\n",
    "    print(\"******************************************************\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vBe402YjF1GW"
   },
   "source": [
    "#### Check the training process and evalute trained model on training and validation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CTvnAINwGM1f"
   },
   "source": [
    "#### Define a function to plot the accuracy stored in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d5-Q38-Q0eGg"
   },
   "outputs": [],
   "source": [
    "def plot_accuracies(history):\n",
    "    plt.plot(history)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.title('Accuracy vs. No. of epochs');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BbeX_bkKGh9A"
   },
   "source": [
    "###Plot training Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FemvDns60izu"
   },
   "outputs": [],
   "source": [
    "#plot_accuracies(trainHistory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EnMwKRL6GoyW"
   },
   "source": [
    "###Ploat Validation Accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_fMU3AJh0qnQ"
   },
   "outputs": [],
   "source": [
    "plot_accuracies(testHistory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u9wV_qrWimKH"
   },
   "outputs": [],
   "source": [
    "plt.plot(lrHistory)\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('LearningRate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xVVIKiC2Gvvi"
   },
   "source": [
    "#### If we want to rerun the training process and want to keep the history of all the training process run below cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Os7cDPXFN--U"
   },
   "outputs": [],
   "source": [
    "plot_accuracies(trainLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-SfNrku8xJJY"
   },
   "outputs": [],
   "source": [
    "trainHistoryFull = []\n",
    "testHistoryFull = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yupzw2cMHGae"
   },
   "source": [
    "#### Comment above sell so we donot accidentally overright it with empty list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CEAPJShn2tt_"
   },
   "outputs": [],
   "source": [
    "for item in trainHistory:\n",
    "  trainHistoryFull.append(item)\n",
    "\n",
    "for item in testHistory:\n",
    "  testHistoryFull.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xElV4lEqxWOs"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(testHistoryFull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NeW2ZIP40OO8"
   },
   "outputs": [],
   "source": [
    "#from matplotlib import pyplot as plt\n",
    "#plt.plot(trainHistoryFull)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TvpKI8uAHo3r"
   },
   "source": [
    "#### Plot and print train error and validation error together to check overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d-EXwvAEDWKd"
   },
   "outputs": [],
   "source": [
    "plt.plot(trainLoss)\n",
    "plt.plot(testLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r1Lg4nGdDYjh"
   },
   "outputs": [],
   "source": [
    "TrainAccuracy = net.evalute(train_X,train_y)\n",
    "EvalAccuracy  = net.evalute(test_X,test_y)\n",
    "print(\"\\nTraining Error   :\" + str(TrainAccuracy))\n",
    "print(\"Validation Error :\" +str(EvalAccuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HHG4FJKB7JMJ"
   },
   "outputs": [],
   "source": [
    "path = \"/content/gdrive/My Drive/Colab Notebooks/TrainedModel/IntelIC757\"\n",
    "torch.save(net, path)\n",
    "#print(f\"Train Error : {trainHistory[-1]}\")\n",
    "#print(f\"Test Error : {testHistory[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GdXVfMqhmi2_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of GPUConvNet.ipynb",
   "provenance": [
    {
     "file_id": "1n4vpRw7cTuIengFp9qFYGzNIasgew1FF",
     "timestamp": 1601188067814
    },
    {
     "file_id": "19JYnsZgO8x1VDc7EqlKZd5zn-y_RjK1I",
     "timestamp": 1596042574503
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
