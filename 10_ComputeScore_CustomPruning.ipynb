{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/thakerpragnesh/DeepLearning/blob/master/10_ComputeScore_CustomPruning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mzhZpUIryCV8",
    "outputId": "b08db130-b2be-4a7a-85ca-063b6411d58b"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_5iaPvtyE33A",
    "tags": []
   },
   "source": [
    "# Important Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Following Cell facilitate to read files from drive and and help to read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "69kf4umwv3pf"
   },
   "outputs": [],
   "source": [
    "#import basic library for some basic function\n",
    "import numpy as np\n",
    "# import library to perform file operation\n",
    "import os #use to access the files \n",
    "import tarfile # use to extract dataset from zip files\n",
    "import sys\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Torch Library provides facilities to create networl architechture and write farword and backwor phase od neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import torch library to build neural network\n",
    "import torch  # Elementory function of tensor is define in torch package\n",
    "import torch.nn as nn # Several layer architectur is define here\n",
    "import torch.nn.functional as F # loss function and activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torchvission provides facilities to access image dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "rlpQUC7qv3pf"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Computer vision is one of the most important application and thus lots \n",
    "of deplopment in the and torch.vision provides many facilities that can \n",
    "be use to imporve model such as data augmentation, reading data batchwise, \n",
    "suffling data before each epoch and many more\n",
    "\"\"\"\n",
    "# import torch library related to image data processing\n",
    "import torchvision # provides facilities to access image dataset\n",
    "from torchvision.datasets.utils import download_url \n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import random_split\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision import datasets, models, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s-U-s2ROL0zF",
    "tags": []
   },
   "source": [
    "# Load the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "51BwTIQdv3pY"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "def ensure_dir(dir_path):\n",
    "    directory = os.path.dirname(dir_path)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "wGKZOAmHwYL0"
   },
   "outputs": [],
   "source": [
    "######################## Input ###############################\n",
    "DatasetLoc = '/home/pragnesh/Dataset/'\n",
    "SelDataSet = 'IntelIC'\n",
    "trainDir = 'train'\n",
    "testDir = 'test'\n",
    "data_dir = DatasetLoc+SelDataSet\n",
    "zipFile = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### Output ##################################\n",
    "#/home3/pragnesh/Dataset/Intel_Image_Classifacation_v2/\n",
    "SelectOutLoc = DatasetLoc+\"Intel_Image_Classifacation_v2/\"  \n",
    "LogLoc =   SelectOutLoc+\"Logs/\"\n",
    "outfile = LogLoc+\"FinalOutv2.log\"\n",
    "logFile = LogLoc+\"ConvModelv2.log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "nslrw6y2wYL1"
   },
   "outputs": [],
   "source": [
    "###################### Model #################################\n",
    "ifLoadModel = False\n",
    "modelname ='vgg16' # vgg11 vgg13 vgg16,  resnet18,  savedmodel\n",
    "ifTransferLearning = True\n",
    "NumberOfClass = 6\n",
    "ModelLoc = SelectOutLoc+\"Model/\"\n",
    "SavePath = SelectOutLoc+'Model/VGG_IntelIC_v1-'+modelname\n",
    "LoadPath = '/content/drive/MyDrive/Model/IntelIC/VGG16IntelIC'#SelectOutLoc+'Model/VGG_IntelIC_v1-'+modelname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "83h1DzDvv3pd"
   },
   "outputs": [],
   "source": [
    "##################### Hyper Parameter #########################\n",
    "\"\"\"\n",
    "***************************************************************\n",
    "We are using one cycle fit function in which learning rate start with 1/10th \n",
    "of selected maximum learning rate and increase learning rate from min to max\n",
    "in 1st phase and then decrease from max to min in 2nd phase\n",
    "***************************************************************\n",
    "Set all the Hyper Parameter Such as \n",
    "1. Learning Rate to control step size\n",
    "2. grad_clip to control the maximum value of gradient\n",
    "3. weight decay to control L2 regularization\n",
    "4. L1 to control L1 regularization\n",
    "5. opt_func to select optimization function\n",
    "***************************************************************\n",
    "\"\"\"\n",
    "max_lr = 1e-3\n",
    "epochs = 1\n",
    "grad_clip = 0.2 \n",
    "weight_decay = 1e-4 \n",
    "L1 = 1e-5\n",
    "opt_func = torch.optim.Adam\n",
    "MODEL_NAME = f\"VGG_Net-\\t MLR-{max_lr}-GC{grad_clip}-WD-{weight_decay}-L1-{L1}\"\n",
    "\n",
    "#################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "bMaNE59Ev3pe"
   },
   "outputs": [],
   "source": [
    "ensure_dir(SelectOutLoc)\n",
    "ensure_dir(ModelLoc)\n",
    "ensure_dir(LogLoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "EHWV7tLxztDE"
   },
   "outputs": [],
   "source": [
    "#Data Prepration\n",
    "\"\"\"\n",
    "Based on the image size of the dataset choose apropriate values of the color channel and Image Size\n",
    "\n",
    "Here we can define path to a folder where we can keep all the dataset. \n",
    "In the following we are using the zip files. Originally dataset should \n",
    "be in the following format DataSetName is parent folder and it should \n",
    "contain train and test folder. train and test folder should contain \n",
    "folder for each category and images of respective category should be in \n",
    "the respective category folder\n",
    "\"\"\"\n",
    "######################### Data Loading #########################################\n",
    "if zipFile == True:\n",
    "  fullpath = data_dir+'.zip'\n",
    "  zip_ref = zipfile.ZipFile(fullpath, 'r') #Opens the zip file in read mode\n",
    "  zip_ref.extractall('/tmp') #Extracts the files into the /tmp folder\n",
    "  data_dir = '/tmp/IntelIC'\n",
    "  testDir ='val'\n",
    "  zip_ref.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create batches of the data and perform transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "IMatzp8Cv3pf"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Choose an apropriate batch size that can be loaded in the current \n",
    "enviroment without crashing and also do not choose too big batch even \n",
    "if dataset is small because it leads to very few updates per epoch\n",
    "\"\"\"\n",
    "#################### Create Batch Of Dataset and do data augmentation ###########\n",
    "bs = 16\n",
    "ImageSize = 224\n",
    "\n",
    "\"\"\"\n",
    "Data Augmentaion generally help in reducing overfitting error during \n",
    "trainng process and thus we are performing randon horizontal flip and \n",
    "random crop during training but during validation as no training happens \n",
    "we dont perform data augmentation\n",
    "\"\"\"\n",
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "data_transforms = {\n",
    "    trainDir: transforms.Compose([\n",
    "        transforms.RandomResizedCrop(ImageSize),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    testDir: transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in [trainDir, testDir]}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=bs,\n",
    "                                             shuffle=True, num_workers=1)\n",
    "              for x in [trainDir, testDir]}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in [trainDir, testDir]}\n",
    "class_names = image_datasets[trainDir].classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6FLIcdgoL-Nv",
    "tags": []
   },
   "source": [
    "# Define Training Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Define functions to facilitate training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "gKlMlgrbv3pg"
   },
   "outputs": [],
   "source": [
    "\"\"\"#Training Process\n",
    "Below code will work as a base function and provide all the important \n",
    "function like compute loss, accuracy and print result in a perticular \n",
    "formate afte each epoch. Funvtion are as follow\n",
    "1. Accuracy : Computer accuracy in evalutaion mode of pytorch on given dataset for given model\n",
    "2. compute_batch_loss : Compute batch loss and append the loss in the list of batch loss.\n",
    "3. compute_batch_loss_acc : Compute batch loss, batch accuracy and append the loss in the list of batch loss.\n",
    "4. accumulate_batch_loss_acc: Accumulate loss from the list of batch and acccuraly loss.\n",
    "5. Epoch end to print the output after every epoch in proper format\n",
    "\"\"\"\n",
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1) \t\t# get the prediction vector\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "# Compute loss of the given batch and return it\n",
    "def compute_batch_loss(newmodel, batch_X,batch_y):\n",
    "  images = batch_X.to(device)\n",
    "  labels = batch_y.to(device)\n",
    "  out = newmodel(images)                  \t\t# Generate predictions\n",
    "  loss = F.cross_entropy(out, labels) \t\t\t# Calculate loss\n",
    "  return loss\n",
    "\n",
    "# Computes loss and accuracy of the given batch(Used in validation)\n",
    "def compute_batch_loss_acc(newmodel, batch_X,batch_y):\n",
    "    images = batch_X.to(device)\n",
    "    labels = batch_y.to(device)\n",
    "    out = newmodel(images)                    \t# Generate predictionsin_features=4096\n",
    "    loss = F.cross_entropy(out, labels)   \t\t# Calculate loss\n",
    "    acc = accuracy(out, labels)           \t\t# Calculate accuracy\n",
    "    return {'val_loss': loss, 'val_acc': acc}\n",
    "\n",
    "# At the end of epoch accumulate all batch loss and batch accueacy    \n",
    "def accumulate_batch_loss_acc(outputs):\n",
    "    batch_losses = [x['val_loss'] for x in outputs]\n",
    "    epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "    batch_accs = [x['val_acc'] for x in outputs]\n",
    "    epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "    return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "\n",
    "def epoch_end(epoch, result):\n",
    "  # Print in given format \n",
    "  # Epoch [0], last_lr: 0.00278, train_loss: 1.2862, val_loss: 1.2110, val_acc: 0.6135\n",
    "  strResult = \"Epoch [{}], last_lr: {:.6f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "      epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_acc'])\n",
    "  #print(strResult)\n",
    "  return strResult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define actual training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Dw5YKsDqv3pg"
   },
   "outputs": [],
   "source": [
    "\"\"\"## Define Training \n",
    "Here we will evalute our model after each epoch on validation dataset using evalute method\n",
    "get_lr method returnd last learning rate used in the training\n",
    "Here we are using one fit cycle method in which we specify the max learning rate and learning \n",
    "rate start from 1/10th value of max_lr and slowly increases the value to max_lr for 40% of updates \n",
    "then decreases to its initial value for 40% updates and then further decreases to 1/100th of max_lr \n",
    "value to perform final fine tuning.\n",
    "\"\"\"\n",
    "# evalute model on given dataset using given data loader\n",
    "@torch.no_grad()\n",
    "# evalute model on given dataset using given data loader\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      for batch_X, batch_y in data_loader:\n",
    "        outputs = [compute_batch_loss_acc(model,batch_X,batch_y)]\n",
    "      return accumulate_batch_loss_acc(outputs)\n",
    "\n",
    "# Use special scheduler to change the value of learning rate\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "# epoch=8, max_lr=.01, weight_decay(L2-Regu parametr)=.0001,opt_func=Adam\n",
    "\n",
    "######### Main Function To Implement Training #################\n",
    "def fit_one_cycle(ModelName,epochs, max_lr, model, \n",
    "                  weight_decay=0, L1=0,grad_clip=None, opt_func=torch.optim.SGD):\n",
    "    torch.cuda.empty_cache()\n",
    "    history = []\n",
    "    # Set up cutom optimizer here we will use one cycle scheduler with max learning\n",
    "    # rate given by max_lr, default optimizer is SGD but we will use ADAM, and \n",
    "    # L2 Regularization using weight decay\n",
    "    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n",
    "    # Set up one-cycle learning rate scheduler\n",
    "    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, \n",
    "                                                steps_per_epoch=len(dataloaders[trainDir]))\n",
    "    print(\"Training Starts\")\n",
    "    with open(logFile, \"a\") as f:\n",
    "      for epoch in range(epochs):\n",
    "          # Training Phase \n",
    "          model.train()  #######################\n",
    "          train_losses = []\n",
    "          lrs = []\n",
    "          #for batch in train_loader:\n",
    "          for batch_X, batch_y in dataloaders[trainDir]:\n",
    "              # computer the training loss of current batch\n",
    "              loss = compute_batch_loss(model,batch_X,batch_y)\n",
    "              l1_crit = nn.L1Loss()\n",
    "              reg_loss = 0\n",
    "              for param in model.parameters():\n",
    "                reg_loss += l1_crit(param,target=torch.zeros_like(param))\n",
    "              loss += L1*reg_loss \n",
    "              \n",
    "              train_losses.append(loss)\n",
    "              loss.backward() # compute the gradient of all weights\n",
    "              # Clip the gradient value to maximum allowed grad_clip value\n",
    "              if grad_clip: \n",
    "                  nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
    "              optimizer.step() # Updates weights \n",
    "              # pytorch by default accumulate grade history and if we dont want it\n",
    "              # we should make all previous grade value equals to zero\n",
    "              optimizer.zero_grad() \n",
    "              # Record & update learning rate\n",
    "              lrs.append(get_lr(optimizer))\n",
    "              sched.step() # Update the learning rate\n",
    "              # Compute Validation Loss and Valodation Accuracy\n",
    "              result = evaluate(model, dataloaders[testDir])\n",
    "              # Compute Train Loss of whole epoch i.e mean of loss of batch \n",
    "              result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "              # Observe how learning rate is change by schedular\n",
    "              result['lrs'] = lrs\n",
    "              # print the observation of each epoch in a proper format\n",
    "          \n",
    "          #strResult = \"Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, \n",
    "            #val_acc: {:.4f}\".format(epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_acc'])\n",
    "          strResult = epoch_end(epoch, result) \n",
    "          \n",
    "          f.write(f\"{ModelName}-\\t{strResult}\\n\")\n",
    "          print(strResult)\n",
    "          history.append(result) # append tupple result with val_acc, vall_loss, and trin_loss\n",
    "        \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check for cuda device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RJXvCs_Rv3ph",
    "outputId": "2b93f483-f009-4fec-f80c-0c77d8902192"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\"\"\"Check if Cuda GPU is available\"\"\"\n",
    "#check for CUDA enabled GPU card\n",
    "def getDeviceType():\n",
    "  if torch.cuda.is_available():\n",
    "    return torch.device('cuda')\n",
    "  else:\n",
    "    return torch.device('cpu')\n",
    "device = getDeviceType()\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Define vggnet architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Define a VggNet Architecture\n",
    "from typing import Union, List, Dict, Any, cast\n",
    "class VGG(nn.Module):\n",
    "    def __init__(\n",
    "        self, features: nn.Module, num_classes: int = 10, init_weights: bool = True, dropout: float = 0.5\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.features = features\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "        if init_weights:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "    def _initialize_weights(self) -> None:\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    \n",
    "def make_layers(cfg: List[Union[str, int]], batch_norm: bool = False) -> nn.Sequential:\n",
    "    layers: List[nn.Module] = []\n",
    "    in_channels = 3\n",
    "    for v in cfg:\n",
    "    #    print(count)\n",
    "        if v == \"M\":\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            v = cast(int, v)\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    #    count+=1\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "def _vgg(VggFeatureList, batch_norm: bool, progress: bool) -> VGG:\n",
    "    \n",
    "    #Calling the constructer here\n",
    "    feature = make_layers(VggFeatureList, batch_norm=batch_norm)\n",
    "    model = VGG( feature )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vgg16Feature=[64, 64, \"M\", 128, 128, \"M\", 256, 256, 256, \"M\", 512, 512, 512, \"M\", 512, 512, 512, \"M\"]\n",
    "vgg16Net = _vgg( vgg16Feature, batch_norm=False, progress=False)\n",
    "print(vgg16Net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Select and load apropriate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "wR7ezXFcv3pj"
   },
   "outputs": [],
   "source": [
    "# In[11]: Load the apropiate model for training\n",
    "############################# Use Pre-Train Model for Transfer Learning #####################################\n",
    "#################### Download Pretrain VGGNet ################\n",
    "def loadPretrainModel(modelname,pretrain: bool = False):\n",
    "    if modelname == 'vgg16' or modelname == 'vgg13' or modelname == 'vgg11':\n",
    "        if modelname == 'vgg11':\n",
    "            newModel = torchvision.models.vgg11(pretrained=False)\n",
    "        if modelname == 'vgg13':\n",
    "            newModel = torchvision.models.vgg13(pretrained=False)\n",
    "        if modelname == 'vgg16':\n",
    "            newModel = torchvision.models.vgg16(pretrained=False)    \n",
    "        if ifTransferLearning:\n",
    "            for param in newModel.parameters():\n",
    "                param.requires_grad = False\n",
    "        #Need to change the below code if we choose different model\n",
    "        print(newModel.classifier[6])\n",
    "        num_ftrs = newModel.classifier[6].in_features\n",
    "        # Here the size of each output sample is set to 10.\n",
    "        # Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
    "        newModel.classifier[6] = nn.Linear(num_ftrs, NumberOfClass)\n",
    "        newModel = newModel.to(device)\n",
    "        return newModel\n",
    "    if modelname == 'vgg16bn' or modelname == 'vgg13bn' or modelname == 'vgg11bn':\n",
    "        if modelname == 'vgg11bn':\n",
    "            newModel = torchvision.models.vgg11_bn(pretrained=False)\n",
    "        if modelname == 'vgg13bn':\n",
    "            newModel = torchvision.models.vgg13_bn(pretrained=False)\n",
    "        if modelname == 'vgg16bn':\n",
    "            newModel = torchvision.models.vgg16_bn(pretrained=False)    \n",
    "        if ifTransferLearning:\n",
    "            for param in newModel.parameters():\n",
    "                param.requires_grad = False\n",
    "        #Need to change the below code if we choose different model\n",
    "        print(newModel.classifier[6])\n",
    "        num_ftrs = newModel.classifier[6].in_features\n",
    "        # Here the size of each output sample is set to 10.\n",
    "        # Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
    "        newModel.classifier[6] = nn.Linear(num_ftrs, NumberOfClass)\n",
    "        newModel = newModel.to(device)\n",
    "        return newModel\n",
    "    ##################### Download Pretrain ResNet 18 ############################\n",
    "    if modelname == 'resnet18':\n",
    "        newModel = torchvision.models.resnet18(pretrained=False)\n",
    "        print(newModel.fc)\n",
    "        for param in newModel.parameters():\n",
    "            param.requires_grad = False\n",
    "        #print(newModel)\n",
    "        #Need to change the below code if we choose different model\n",
    "        num_ftrs = newModel.fc.in_features\n",
    "        # Here the size of each output sample is set to 10.\n",
    "        # Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
    "        newModel.fc = nn.Linear(num_ftrs, 10)\n",
    "        newModel = newModel.to(device)\n",
    "        return newModel\n",
    "\n",
    "################# Load Stored Trained Model #####################################\n",
    "def loadSavedModel(LoadPath,device=torch.device('cpu')):\n",
    "    if device== torch.device('cpu'):\n",
    "        newModel = torch.load(LoadPath, map_location=torch.device('cpu'))\n",
    "    else:\n",
    "        newModel = torch.load(LoadPath, map_location=torch.device('cuda'))\n",
    "\n",
    "def freezeFeature(modelname,newModel):\n",
    "    count=0\n",
    "    if modelname == 'VGG16':\n",
    "        for param in newModel.parameters():\n",
    "            if count in (26,28,30):\n",
    "              param.requires_grad=True\n",
    "            else:\n",
    "              param.requires_grad=False\n",
    "                \n",
    "    if modelname == 'VGG13':\n",
    "        for param in newModel.parameters():\n",
    "            if count in (20,22,24):\n",
    "              param.requires_grad=True\n",
    "            else:\n",
    "              param.requires_grad=False\n",
    "            \n",
    "    if modelname == 'VGG11':\n",
    "        for param in newModel.parameters():\n",
    "            if count in (16,18,20):\n",
    "              param.requires_grad=True\n",
    "            else:\n",
    "              param.requires_grad=False\n",
    "    \n",
    "def freeze(modelname,newModel):\n",
    "    count=0\n",
    "    if modelname == 'VGG16':\n",
    "        for param in newModel.parameters():\n",
    "            if count == 30:\n",
    "              param.requires_grad=True\n",
    "            else:\n",
    "              param.requires_grad=False\n",
    "                \n",
    "    if modelname == 'VGG13':\n",
    "        for param in newModel.parameters():\n",
    "            if count == 24:\n",
    "              param.requires_grad=True\n",
    "            else:\n",
    "              param.requires_grad=False\n",
    "            \n",
    "    if modelname == 'VGG11':\n",
    "        for param in newModel.parameters():\n",
    "            if count == 20:\n",
    "              param.requires_grad=True\n",
    "            else:\n",
    "              param.requires_grad=False\n",
    "            \n",
    "def unFreaze(newModel):\n",
    "    for param in newModel.parameters():\n",
    "        param.requires_grad=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=4096, out_features=1000, bias=True)\n"
     ]
    }
   ],
   "source": [
    "if ifLoadModel:\n",
    "    newModel = loadSavedModel(LoadPath)\n",
    "    freezeFeature(modelname,newModel)\n",
    "else:\n",
    "    newModel = loadPretrainModel(modelname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=6, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(newModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6LSml7L73hr8",
    "tags": []
   },
   "source": [
    "\n",
    "# Perform the training\n",
    "Start the training using one fit cycle algorithms where learning rate start with one tenth of provided max_lr and then increase it value till that point and then decrease onword and for last few epoch learning rate furthe decrease\n",
    "Pass the hyperparameter for training of the model and start training process for set number of epcoh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P9XcNY6R25K4"
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "historylast=[]\n",
    "historylast += fit_one_cycle(MODEL_NAME,epochs, max_lr, newModel,  \n",
    "                              grad_clip=grad_clip, \n",
    "                              weight_decay=weight_decay, L1=L1,\n",
    "                              opt_func=opt_func\n",
    "                              )\n",
    "################################################ Evalute the Training Process by Plotting Graph ###################################\n",
    "torch.save(newModel, SavePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_41Q-qkBtT_l",
    "tags": []
   },
   "source": [
    "## Evalute the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "uJG3Ga9bv3pj"
   },
   "outputs": [],
   "source": [
    "\"\"\"##Define method to compute accuracy for a given model on given dataset\"\"\"\n",
    "def accuraciesTotal(newModel, data_loader):\n",
    "  with torch.no_grad():\n",
    "    acc = []\n",
    "    for batch in data_loader:\n",
    "        images, label = batch\n",
    "        images, labels = batch[0].to(device), batch[1].to(device)\n",
    "        out = newModel(images)\n",
    "        acc.append(accuracy(out, labels))\n",
    "    return torch.mean(torch.stack(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare dataset for evaluation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "J8_5NMXkv3pk"
   },
   "outputs": [],
   "source": [
    "### Prepare the data loader for inference. During the \n",
    "### inference we want to perform same tranformation on test and train dataset\"\"\"\n",
    "image_datasets_eval = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[testDir])\n",
    "                  for x in [trainDir, testDir]}\n",
    "dataloaders_eval = {x: torch.utils.data.DataLoader(image_datasets_eval[x], batch_size=bs,\n",
    "                                             shuffle=False, num_workers=1)\n",
    "                  for x in [trainDir, testDir]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evalute the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 257
    },
    "id": "XzZrD_IHtqA8",
    "outputId": "d888f895-9039-4f0b-8fe3-1aa0d257d871"
   },
   "outputs": [],
   "source": [
    "# In[17]: Evalutute train and test error\n",
    "trainacc = 0\n",
    "testacc = 0\n",
    "trainacc = accuraciesTotal(newModel,dataloaders_eval[trainDir])\n",
    "testacc = accuraciesTotal(newModel,dataloaders_eval[testDir])\n",
    "\n",
    "with open(outfile,'a') as f:\n",
    "    f.write(f\"Train Accuracy :  {trainacc}\\n Test Accuracy  :  {testacc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "536iqc9dtbck",
    "tags": []
   },
   "source": [
    "# Pruning Process\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I5AIO_gI3LXo"
   },
   "source": [
    "In the below code we create a list module with candidate layer to prune with their respective probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(newModel.features)):\n",
    "    if str(newModel.features[i]).find('Conv') != -1:\n",
    "        print(f\"Index:{i} Layer:{newModel.features[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import my_utils.initialize_pruning as ip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3vl96xL8v3pk",
    "outputId": "65753b26-c923-4e04-cc08-6b78009778fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block List   = [2, 2, 3, 3, 3]\n",
      "Feature List = [0, 2, 5, 7, 10, 12, 14, 17, 19, 21, 24, 26, 28]\n",
      "Prune Count  = [1, 1, 3, 3, 9, 9, 9, 26, 26, 26, 51, 51, 51]\n",
      "0 : torch.Size([64, 3, 3, 3])\n",
      "1 : torch.Size([64, 64, 3, 3])\n",
      "2 : torch.Size([128, 64, 3, 3])\n",
      "3 : torch.Size([128, 128, 3, 3])\n",
      "4 : torch.Size([256, 128, 3, 3])\n",
      "5 : torch.Size([256, 256, 3, 3])\n",
      "6 : torch.Size([256, 256, 3, 3])\n",
      "7 : torch.Size([512, 256, 3, 3])\n",
      "8 : torch.Size([512, 512, 3, 3])\n",
      "9 : torch.Size([512, 512, 3, 3])\n",
      "10 : torch.Size([512, 512, 3, 3])\n",
      "11 : torch.Size([512, 512, 3, 3])\n",
      "12 : torch.Size([512, 512, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Get the indices of the different model that gives parameter to be prune\n",
    "'''\n",
    "        \n",
    "Module = ip.getPruneModule(newModel)\n",
    "blockList = ip.createBlockList(newModel)\n",
    "featureList = ip.createFeatureList(newModel)\n",
    "pr = ip.getPruneCount(module=Module,blocks=blockList,maxpr=.1)\n",
    "\n",
    "# for i in range(len(pr)):\n",
    "#     size = Module[i]._parameters['weight'].shape\n",
    "#     c = int(round(size[0]*pr[i]))\n",
    "#     prune_count.append(c)\n",
    "#     print('%.4f  '%pr[i],end=\"\")\n",
    "\n",
    "print(\"Block List   =\",blockList)\n",
    "print(\"Feature List =\",featureList)\n",
    "print(\"Prune Count  =\",pr)\n",
    "for i in range(len(Module)):\n",
    "        print(f\"{i} : {Module[i]._parameters['weight'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g27mZ0Jg_qUy"
   },
   "source": [
    "Compute the distance betweeen each pair of kernal working on same input channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "x9zeSe1xv3pl"
   },
   "outputs": [],
   "source": [
    "#t = Tensor to be prune, n is ln normalization, dim dimension over which we want to perform \n",
    "def _compute_distance_score(t, n=1, dim_to_keep=[0,1],threshold=1):\n",
    "        # dims = all axes, except for the one identified by `dim`        \n",
    "        dim_to_prune = list(range(t.dim()))   #initially it has all dims\n",
    "        #remove dim which we want to keep from dimstoprune\n",
    "        for i in range(len(dim_to_keep)):   \n",
    "            dim_to_prune.remove(dim_to_keep[i])\n",
    "        \n",
    "        size = t.shape\n",
    "        print(f\"\\nShape of the tensor: {size}\")\n",
    "        print(f\"Print the Dims we want to keep: {dim_to_keep}\")\n",
    "        \n",
    "        module_buffer = torch.zeros_like(t)\n",
    "                \n",
    "        #shape of norm should be equal to multiplication of dim to keep values\n",
    "        norm = torch.norm(t, p=n, dim=dim_to_prune)\n",
    "        print(f\"norm shape = {norm.shape}\")\n",
    "        size = t.shape\n",
    "        print(\"Number Of Features Map in current  layer l     =\",size[0])\n",
    "        print(\"Number Of Features Map in previous layer (l-1) =\",size[1])\n",
    "        \n",
    "        for i in range(size[0]):\n",
    "            for j in range(size[1]):\n",
    "                module_buffer[i][j] = t[i][j]/norm[i][j]\n",
    "        \n",
    "        dist = torch.zeros(size[1],size[0],size[0])\n",
    "        \n",
    "        channelList = []\n",
    "        for j in range(size[1]):\n",
    "            idxtupple = []\n",
    "            print('.',end='')\n",
    "            for i1 in range(size[0]):\n",
    "                for i2 in range((i1+1),size[0]):\n",
    "                    dist[j][i1][i2] = torch.norm( (module_buffer[i1][j]-module_buffer[i2][j]) ,p=1)\n",
    "                    dist[j][i2][i1] = dist[j][i1][i2]\n",
    "                    \n",
    "                    if dist[j][i1][i2] < threshold:\n",
    "                        idxtupple.append([j,i1,i2,dist[j][i1][i2]])\n",
    "            channelList.append(idxtupple)\n",
    "        return channelList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h64rdZml_6Y9"
   },
   "source": [
    "Compute the Cummulative value of each kernal working on same input channel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "r7fTZ0Qpv3pm"
   },
   "outputs": [],
   "source": [
    "#t = Tensor to be prune, n is ln normalization, dim dimension over which we want to perform \n",
    "def _compute_kernal_score(t, n=1, dim_to_keep=[0,1],threshold=1):\n",
    "        # dims = all axes, except for the one identified by `dim`        \n",
    "        dim_to_prune = list(range(t.dim()))   #initially it has all dims\n",
    "        #remove dim which we want to keep from dimstoprune\n",
    "        for i in range(len(dim_to_keep)):   \n",
    "            dim_to_prune.remove(dim_to_keep[i])\n",
    "        \n",
    "        size = t.shape\n",
    "        print(size)\n",
    "        print(dim_to_keep)\n",
    "        \n",
    "        module_buffer = torch.zeros_like(t)\n",
    "                \n",
    "        #shape of norm should be equal to multiplication of dim to keep values\n",
    "        norm = torch.norm(t, p=n, dim=dim_to_prune)\n",
    "        return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of the tensor: torch.Size([64, 3, 3, 3])\n",
      "Print the Dims we want to keep: [0, 1]\n",
      "norm shape = torch.Size([64, 3])\n",
      "Number Of Features Map in current  layer l     = 64\n",
      "Number Of Features Map in previous layer (l-1) = 3\n",
      "...\n",
      "Shape of the tensor: torch.Size([64, 64, 3, 3])\n",
      "Print the Dims we want to keep: [0, 1]\n",
      "norm shape = torch.Size([64, 64])\n",
      "Number Of Features Map in current  layer l     = 64\n",
      "Number Of Features Map in previous layer (l-1) = 64\n",
      "................................................................\n",
      "Shape of the tensor: torch.Size([128, 64, 3, 3])\n",
      "Print the Dims we want to keep: [0, 1]\n",
      "norm shape = torch.Size([128, 64])\n",
      "Number Of Features Map in current  layer l     = 128\n",
      "Number Of Features Map in previous layer (l-1) = 64\n",
      "................................................................\n",
      "Shape of the tensor: torch.Size([128, 128, 3, 3])\n",
      "Print the Dims we want to keep: [0, 1]\n",
      "norm shape = torch.Size([128, 128])\n",
      "Number Of Features Map in current  layer l     = 128\n",
      "Number Of Features Map in previous layer (l-1) = 128\n",
      "................................................................................................................................\n",
      "\n",
      "\n",
      "Here is the : 4\n"
     ]
    }
   ],
   "source": [
    "channelTuppleList = []\n",
    "st =0\n",
    "en = 4\n",
    "for i in range(st,en):\n",
    "    channelTuppleList.append(_compute_distance_score(Module[i]._parameters['weight'],threshold=1))\n",
    "print(\"\\n\\n\\nHere is the :\",len(channelTuppleList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "6rYoTe5v5-gb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of filter in Channel [0][0] below threshold: 163\n",
      "Number of filter in Channel [0][1] below threshold: 176\n",
      "Number of filter in Channel [0][2] below threshold: 153\n",
      "Number of filter in Channel [1][0] below threshold: 170\n",
      "Number of filter in Channel [1][1] below threshold: 157\n",
      "Number of filter in Channel [1][2] below threshold: 164\n",
      "Number of filter in Channel [1][3] below threshold: 183\n",
      "Number of filter in Channel [1][4] below threshold: 176\n",
      "Number of filter in Channel [1][5] below threshold: 154\n",
      "Number of filter in Channel [1][6] below threshold: 167\n",
      "Number of filter in Channel [1][7] below threshold: 176\n",
      "Number of filter in Channel [1][8] below threshold: 161\n",
      "Number of filter in Channel [1][9] below threshold: 159\n",
      "Number of filter in Channel [1][10] below threshold: 170\n",
      "Number of filter in Channel [1][11] below threshold: 158\n",
      "Number of filter in Channel [1][12] below threshold: 189\n",
      "Number of filter in Channel [1][13] below threshold: 135\n",
      "Number of filter in Channel [1][14] below threshold: 170\n",
      "Number of filter in Channel [1][15] below threshold: 171\n",
      "Number of filter in Channel [1][16] below threshold: 127\n",
      "Number of filter in Channel [1][17] below threshold: 165\n",
      "Number of filter in Channel [1][18] below threshold: 157\n",
      "Number of filter in Channel [1][19] below threshold: 151\n",
      "Number of filter in Channel [1][20] below threshold: 195\n",
      "Number of filter in Channel [1][21] below threshold: 174\n",
      "Number of filter in Channel [1][22] below threshold: 153\n",
      "Number of filter in Channel [1][23] below threshold: 170\n",
      "Number of filter in Channel [1][24] below threshold: 170\n",
      "Number of filter in Channel [1][25] below threshold: 213\n",
      "Number of filter in Channel [1][26] below threshold: 157\n",
      "Number of filter in Channel [1][27] below threshold: 190\n",
      "Number of filter in Channel [1][28] below threshold: 129\n",
      "Number of filter in Channel [1][29] below threshold: 180\n",
      "Number of filter in Channel [1][30] below threshold: 145\n",
      "Number of filter in Channel [1][31] below threshold: 154\n",
      "Number of filter in Channel [1][32] below threshold: 196\n",
      "Number of filter in Channel [1][33] below threshold: 176\n",
      "Number of filter in Channel [1][34] below threshold: 135\n",
      "Number of filter in Channel [1][35] below threshold: 163\n",
      "Number of filter in Channel [1][36] below threshold: 175\n",
      "Number of filter in Channel [1][37] below threshold: 177\n",
      "Number of filter in Channel [1][38] below threshold: 220\n",
      "Number of filter in Channel [1][39] below threshold: 171\n",
      "Number of filter in Channel [1][40] below threshold: 165\n",
      "Number of filter in Channel [1][41] below threshold: 166\n",
      "Number of filter in Channel [1][42] below threshold: 147\n",
      "Number of filter in Channel [1][43] below threshold: 166\n",
      "Number of filter in Channel [1][44] below threshold: 161\n",
      "Number of filter in Channel [1][45] below threshold: 190\n",
      "Number of filter in Channel [1][46] below threshold: 166\n",
      "Number of filter in Channel [1][47] below threshold: 207\n",
      "Number of filter in Channel [1][48] below threshold: 190\n",
      "Number of filter in Channel [1][49] below threshold: 146\n",
      "Number of filter in Channel [1][50] below threshold: 175\n",
      "Number of filter in Channel [1][51] below threshold: 176\n",
      "Number of filter in Channel [1][52] below threshold: 170\n",
      "Number of filter in Channel [1][53] below threshold: 165\n",
      "Number of filter in Channel [1][54] below threshold: 176\n",
      "Number of filter in Channel [1][55] below threshold: 156\n",
      "Number of filter in Channel [1][56] below threshold: 178\n",
      "Number of filter in Channel [1][57] below threshold: 143\n",
      "Number of filter in Channel [1][58] below threshold: 158\n",
      "Number of filter in Channel [1][59] below threshold: 171\n",
      "Number of filter in Channel [1][60] below threshold: 138\n",
      "Number of filter in Channel [1][61] below threshold: 158\n",
      "Number of filter in Channel [1][62] below threshold: 159\n",
      "Number of filter in Channel [1][63] below threshold: 162\n",
      "Number of filter in Channel [2][0] below threshold: 712\n",
      "Number of filter in Channel [2][1] below threshold: 625\n",
      "Number of filter in Channel [2][2] below threshold: 701\n",
      "Number of filter in Channel [2][3] below threshold: 664\n",
      "Number of filter in Channel [2][4] below threshold: 678\n",
      "Number of filter in Channel [2][5] below threshold: 722\n",
      "Number of filter in Channel [2][6] below threshold: 628\n",
      "Number of filter in Channel [2][7] below threshold: 688\n",
      "Number of filter in Channel [2][8] below threshold: 630\n",
      "Number of filter in Channel [2][9] below threshold: 633\n",
      "Number of filter in Channel [2][10] below threshold: 678\n",
      "Number of filter in Channel [2][11] below threshold: 578\n",
      "Number of filter in Channel [2][12] below threshold: 597\n",
      "Number of filter in Channel [2][13] below threshold: 624\n",
      "Number of filter in Channel [2][14] below threshold: 654\n",
      "Number of filter in Channel [2][15] below threshold: 645\n",
      "Number of filter in Channel [2][16] below threshold: 767\n",
      "Number of filter in Channel [2][17] below threshold: 631\n",
      "Number of filter in Channel [2][18] below threshold: 685\n",
      "Number of filter in Channel [2][19] below threshold: 649\n",
      "Number of filter in Channel [2][20] below threshold: 651\n",
      "Number of filter in Channel [2][21] below threshold: 730\n",
      "Number of filter in Channel [2][22] below threshold: 752\n",
      "Number of filter in Channel [2][23] below threshold: 669\n",
      "Number of filter in Channel [2][24] below threshold: 695\n",
      "Number of filter in Channel [2][25] below threshold: 659\n",
      "Number of filter in Channel [2][26] below threshold: 652\n",
      "Number of filter in Channel [2][27] below threshold: 665\n",
      "Number of filter in Channel [2][28] below threshold: 751\n",
      "Number of filter in Channel [2][29] below threshold: 709\n",
      "Number of filter in Channel [2][30] below threshold: 689\n",
      "Number of filter in Channel [2][31] below threshold: 577\n",
      "Number of filter in Channel [2][32] below threshold: 690\n",
      "Number of filter in Channel [2][33] below threshold: 636\n",
      "Number of filter in Channel [2][34] below threshold: 672\n",
      "Number of filter in Channel [2][35] below threshold: 666\n",
      "Number of filter in Channel [2][36] below threshold: 681\n",
      "Number of filter in Channel [2][37] below threshold: 710\n",
      "Number of filter in Channel [2][38] below threshold: 635\n",
      "Number of filter in Channel [2][39] below threshold: 694\n",
      "Number of filter in Channel [2][40] below threshold: 727\n",
      "Number of filter in Channel [2][41] below threshold: 735\n",
      "Number of filter in Channel [2][42] below threshold: 624\n",
      "Number of filter in Channel [2][43] below threshold: 699\n",
      "Number of filter in Channel [2][44] below threshold: 658\n",
      "Number of filter in Channel [2][45] below threshold: 683\n",
      "Number of filter in Channel [2][46] below threshold: 675\n",
      "Number of filter in Channel [2][47] below threshold: 806\n",
      "Number of filter in Channel [2][48] below threshold: 725\n",
      "Number of filter in Channel [2][49] below threshold: 631\n",
      "Number of filter in Channel [2][50] below threshold: 624\n",
      "Number of filter in Channel [2][51] below threshold: 706\n",
      "Number of filter in Channel [2][52] below threshold: 687\n",
      "Number of filter in Channel [2][53] below threshold: 698\n",
      "Number of filter in Channel [2][54] below threshold: 779\n",
      "Number of filter in Channel [2][55] below threshold: 714\n",
      "Number of filter in Channel [2][56] below threshold: 689\n",
      "Number of filter in Channel [2][57] below threshold: 763\n",
      "Number of filter in Channel [2][58] below threshold: 663\n",
      "Number of filter in Channel [2][59] below threshold: 667\n",
      "Number of filter in Channel [2][60] below threshold: 666\n",
      "Number of filter in Channel [2][61] below threshold: 746\n",
      "Number of filter in Channel [2][62] below threshold: 638\n",
      "Number of filter in Channel [2][63] below threshold: 652\n",
      "Number of filter in Channel [3][0] below threshold: 690\n",
      "Number of filter in Channel [3][1] below threshold: 693\n",
      "Number of filter in Channel [3][2] below threshold: 675\n",
      "Number of filter in Channel [3][3] below threshold: 605\n",
      "Number of filter in Channel [3][4] below threshold: 683\n",
      "Number of filter in Channel [3][5] below threshold: 753\n",
      "Number of filter in Channel [3][6] below threshold: 689\n",
      "Number of filter in Channel [3][7] below threshold: 686\n",
      "Number of filter in Channel [3][8] below threshold: 663\n",
      "Number of filter in Channel [3][9] below threshold: 671\n",
      "Number of filter in Channel [3][10] below threshold: 648\n",
      "Number of filter in Channel [3][11] below threshold: 694\n",
      "Number of filter in Channel [3][12] below threshold: 646\n",
      "Number of filter in Channel [3][13] below threshold: 715\n",
      "Number of filter in Channel [3][14] below threshold: 641\n",
      "Number of filter in Channel [3][15] below threshold: 730\n",
      "Number of filter in Channel [3][16] below threshold: 705\n",
      "Number of filter in Channel [3][17] below threshold: 743\n",
      "Number of filter in Channel [3][18] below threshold: 707\n",
      "Number of filter in Channel [3][19] below threshold: 628\n",
      "Number of filter in Channel [3][20] below threshold: 650\n",
      "Number of filter in Channel [3][21] below threshold: 689\n",
      "Number of filter in Channel [3][22] below threshold: 570\n",
      "Number of filter in Channel [3][23] below threshold: 632\n",
      "Number of filter in Channel [3][24] below threshold: 778\n",
      "Number of filter in Channel [3][25] below threshold: 697\n",
      "Number of filter in Channel [3][26] below threshold: 682\n",
      "Number of filter in Channel [3][27] below threshold: 725\n",
      "Number of filter in Channel [3][28] below threshold: 667\n",
      "Number of filter in Channel [3][29] below threshold: 742\n",
      "Number of filter in Channel [3][30] below threshold: 674\n",
      "Number of filter in Channel [3][31] below threshold: 645\n",
      "Number of filter in Channel [3][32] below threshold: 674\n",
      "Number of filter in Channel [3][33] below threshold: 657\n",
      "Number of filter in Channel [3][34] below threshold: 694\n",
      "Number of filter in Channel [3][35] below threshold: 717\n",
      "Number of filter in Channel [3][36] below threshold: 747\n",
      "Number of filter in Channel [3][37] below threshold: 682\n",
      "Number of filter in Channel [3][38] below threshold: 725\n",
      "Number of filter in Channel [3][39] below threshold: 604\n",
      "Number of filter in Channel [3][40] below threshold: 643\n",
      "Number of filter in Channel [3][41] below threshold: 687\n",
      "Number of filter in Channel [3][42] below threshold: 678\n",
      "Number of filter in Channel [3][43] below threshold: 624\n",
      "Number of filter in Channel [3][44] below threshold: 666\n",
      "Number of filter in Channel [3][45] below threshold: 649\n",
      "Number of filter in Channel [3][46] below threshold: 676\n",
      "Number of filter in Channel [3][47] below threshold: 676\n",
      "Number of filter in Channel [3][48] below threshold: 685\n",
      "Number of filter in Channel [3][49] below threshold: 718\n",
      "Number of filter in Channel [3][50] below threshold: 642\n",
      "Number of filter in Channel [3][51] below threshold: 704\n",
      "Number of filter in Channel [3][52] below threshold: 815\n",
      "Number of filter in Channel [3][53] below threshold: 695\n",
      "Number of filter in Channel [3][54] below threshold: 782\n",
      "Number of filter in Channel [3][55] below threshold: 724\n",
      "Number of filter in Channel [3][56] below threshold: 755\n",
      "Number of filter in Channel [3][57] below threshold: 576\n",
      "Number of filter in Channel [3][58] below threshold: 620\n",
      "Number of filter in Channel [3][59] below threshold: 768\n",
      "Number of filter in Channel [3][60] below threshold: 678\n",
      "Number of filter in Channel [3][61] below threshold: 683\n",
      "Number of filter in Channel [3][62] below threshold: 646\n",
      "Number of filter in Channel [3][63] below threshold: 744\n",
      "Number of filter in Channel [3][64] below threshold: 681\n",
      "Number of filter in Channel [3][65] below threshold: 664\n",
      "Number of filter in Channel [3][66] below threshold: 660\n",
      "Number of filter in Channel [3][67] below threshold: 699\n",
      "Number of filter in Channel [3][68] below threshold: 634\n",
      "Number of filter in Channel [3][69] below threshold: 696\n",
      "Number of filter in Channel [3][70] below threshold: 694\n",
      "Number of filter in Channel [3][71] below threshold: 690\n",
      "Number of filter in Channel [3][72] below threshold: 747\n",
      "Number of filter in Channel [3][73] below threshold: 651\n",
      "Number of filter in Channel [3][74] below threshold: 702\n",
      "Number of filter in Channel [3][75] below threshold: 654\n",
      "Number of filter in Channel [3][76] below threshold: 646\n",
      "Number of filter in Channel [3][77] below threshold: 688\n",
      "Number of filter in Channel [3][78] below threshold: 652\n",
      "Number of filter in Channel [3][79] below threshold: 618\n",
      "Number of filter in Channel [3][80] below threshold: 704\n",
      "Number of filter in Channel [3][81] below threshold: 652\n",
      "Number of filter in Channel [3][82] below threshold: 678\n",
      "Number of filter in Channel [3][83] below threshold: 713\n",
      "Number of filter in Channel [3][84] below threshold: 669\n",
      "Number of filter in Channel [3][85] below threshold: 595\n",
      "Number of filter in Channel [3][86] below threshold: 755\n",
      "Number of filter in Channel [3][87] below threshold: 720\n",
      "Number of filter in Channel [3][88] below threshold: 650\n",
      "Number of filter in Channel [3][89] below threshold: 656\n",
      "Number of filter in Channel [3][90] below threshold: 680\n",
      "Number of filter in Channel [3][91] below threshold: 720\n",
      "Number of filter in Channel [3][92] below threshold: 608\n",
      "Number of filter in Channel [3][93] below threshold: 675\n",
      "Number of filter in Channel [3][94] below threshold: 698\n",
      "Number of filter in Channel [3][95] below threshold: 728\n",
      "Number of filter in Channel [3][96] below threshold: 662\n",
      "Number of filter in Channel [3][97] below threshold: 773\n",
      "Number of filter in Channel [3][98] below threshold: 702\n",
      "Number of filter in Channel [3][99] below threshold: 588\n",
      "Number of filter in Channel [3][100] below threshold: 695\n",
      "Number of filter in Channel [3][101] below threshold: 684\n",
      "Number of filter in Channel [3][102] below threshold: 640\n",
      "Number of filter in Channel [3][103] below threshold: 618\n",
      "Number of filter in Channel [3][104] below threshold: 696\n",
      "Number of filter in Channel [3][105] below threshold: 713\n",
      "Number of filter in Channel [3][106] below threshold: 706\n",
      "Number of filter in Channel [3][107] below threshold: 594\n",
      "Number of filter in Channel [3][108] below threshold: 693\n",
      "Number of filter in Channel [3][109] below threshold: 704\n",
      "Number of filter in Channel [3][110] below threshold: 676\n",
      "Number of filter in Channel [3][111] below threshold: 669\n",
      "Number of filter in Channel [3][112] below threshold: 638\n",
      "Number of filter in Channel [3][113] below threshold: 664\n",
      "Number of filter in Channel [3][114] below threshold: 614\n",
      "Number of filter in Channel [3][115] below threshold: 691\n",
      "Number of filter in Channel [3][116] below threshold: 643\n",
      "Number of filter in Channel [3][117] below threshold: 652\n",
      "Number of filter in Channel [3][118] below threshold: 644\n",
      "Number of filter in Channel [3][119] below threshold: 701\n",
      "Number of filter in Channel [3][120] below threshold: 633\n",
      "Number of filter in Channel [3][121] below threshold: 636\n",
      "Number of filter in Channel [3][122] below threshold: 677\n",
      "Number of filter in Channel [3][123] below threshold: 664\n",
      "Number of filter in Channel [3][124] below threshold: 713\n",
      "Number of filter in Channel [3][125] below threshold: 623\n",
      "Number of filter in Channel [3][126] below threshold: 694\n",
      "Number of filter in Channel [3][127] below threshold: 668\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(channelTuppleList)):\n",
    "    for j in range(len(channelTuppleList[i])):\n",
    "        print(f\"Number of filter in Channel [{i}][{j}] below threshold: {len(channelTuppleList[i][j])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "TPR4goobv3pn"
   },
   "outputs": [],
   "source": [
    "def displayLayer(channelTupple):\n",
    "    for i in range(len(channelTupple)):\n",
    "        for j in range(len(channelTupple[i])):\n",
    "            if j%3==0:\n",
    "                print()\n",
    "        print(channelTupple[i][j],'\\t',end='')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "hQjzH9-68oMa"
   },
   "outputs": [],
   "source": [
    "def makeChannelList(Module,startIdx,endIdx,thresh_hold):\n",
    "    channelTuppleList = []\n",
    "    for i in range(st,en):\n",
    "        channelTuppleList.append(_compute_distance_score(Module[i]._parameters['weight'],threshold=thresh_hold))\n",
    "    print(\"\\n\\n\\nHere is the :\",len(channelTuppleList))\n",
    "    return channelTuppleList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "_MR_v8TQv3pn"
   },
   "outputs": [],
   "source": [
    "def sortKernal(kernalList):\n",
    "  for i in range(len(kernalList)):\n",
    "    iListLen = len(kernalList[i])\n",
    "    #print(f'lemgth of list {i} ={iListLen}')\n",
    "    for j in range(iListLen):\n",
    "      for k in range(iListLen-j-1):\n",
    "        #print(f\"Value of i={i}     Value of j={j} Value of k={k}\")\n",
    "        if kernalList[i][k+1][3] < kernalList[i][k][3]:\n",
    "          kernalList[i][k+1], kernalList[i][k] = kernalList[i][k], kernalList[i][k+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "cK-kwA8V_Cs-"
   },
   "outputs": [],
   "source": [
    "def get_k_element(channel_list,k):\n",
    "    channel_k_list = []\n",
    "    for i in range(len(channelTuppleList)):\n",
    "    #print(i)\n",
    "        for j in range(k):\n",
    "            channel_k_list.append(channel_list[i][j])\n",
    "    return channel_k_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(channelTuppleList))\n",
    "for i in range(3):\n",
    "    print(len(channelTuppleList[i]))\n",
    "    for j in range(3):\n",
    "        print(len(channelTuppleList[i][j]))\n",
    "        for k in range(3):\n",
    "            print(len(ch))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TZdUuOt0v3pn"
   },
   "outputs": [],
   "source": [
    "for i in range(len(channelTuppleList)):\n",
    "  sortKernal(channelTuppleList[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qgB79d4QDiqj"
   },
   "outputs": [],
   "source": [
    "newList = []\n",
    "for i in range(len(channelTuppleList)):\n",
    "    newList.append(get_k_element(channel_list=channelTuppleList[i],k=3) )\n",
    "for i in range(len(newList)):\n",
    "    print(len(newList[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(newList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tYZqHetC3w4G"
   },
   "outputs": [],
   "source": [
    "for i in range(len(channelTuppleList)):\n",
    "  print(\"\\n**************************************************************************************************************************\")\n",
    "  displayLayer(channelTuppleList[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dkVtjlyjv3po"
   },
   "source": [
    "## Pruning Start From Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Pruning Class and Pruning Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.prune as prune\n",
    "class KernalPruningMethod(prune.BasePruningMethod):\n",
    "    \"\"\"Prune every other entry in a tensor\n",
    "    \"\"\"\n",
    "    PRUNING_TYPE = 'unstructured'\n",
    "\n",
    "    def compute_mask(self, t, default_mask):\n",
    "        mask = default_mask.clone()\n",
    "        #mask.view(-1)[::2] = 0\n",
    "        size = t.shape\n",
    "        print(size)\n",
    "        for k1 in range(len(newList[layer_number])):\n",
    "            for k2 in range(len(newList[layer_number][k1])):\n",
    "                i= newList[layer_number][k1][k2][1]\n",
    "                j= newList[layer_number][k1][k2][0]\n",
    "                #print(f'i={i}   j={j}')\n",
    "                mask[i][j] = 0\n",
    "        return mask\n",
    "def kernal_unstructured(module, name):\n",
    "    KernalPruningMethod.apply(module, name)\n",
    "    return module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform pruning on list of Mudele using custom pruning method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_number = 0\n",
    "for i in range(3):\n",
    "    kernal_unstructured(Module[i], name = 'weight')\n",
    "    layer_number += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Commit the pruning and remove all buffer associated with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    prune.remove(Module[i], 'weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write down deep Copy Code Which Copy Weights Of The Kernal That copy weights which have non zero weight in kernal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "10 ComputeScore-CustomPruning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
